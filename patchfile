diff --git a/ci/platform/alembic/versions/10b0fad87127_removing_index_uniqu.py b/ci/platform/alembic/versions/10b0fad87127_removing_index_uniqu.py
index 81f917d..f3d7043 100644
--- a/ci/platform/alembic/versions/10b0fad87127_removing_index_uniqu.py
+++ b/ci/platform/alembic/versions/10b0fad87127_removing_index_uniqu.py
@@ -26,9 +26,9 @@ def downgrade():
     op.drop_index('ix_console_output_build_console_id', 'console_output')
     remove_dupes = """DELETE FROM console_output WHERE id IN (
         SELECT id FROM (
-            SELECT
+            SELECT 
                 id, ROW_NUMBER() OVER(PARTITION BY build_console_id, line_number ORDER BY id DESC) AS row_num
-            FROM
+            FROM 
                 console_output
         ) AS unduped WHERE row_num > 1)"""
 
diff --git a/ci/platform/alembic/versions/2b278a305caf_add_github_commit_ow.py b/ci/platform/alembic/versions/2b278a305caf_add_github_commit_ow.py
deleted file mode 100644
index 2f624a3..0000000
--- a/ci/platform/alembic/versions/2b278a305caf_add_github_commit_ow.py
+++ /dev/null
@@ -1,28 +0,0 @@
-"""Add github commit owner data
-
-Revision ID: 2b278a305caf
-Revises: 31a259340697
-Create Date: 2013-09-12 13:06:24.408509
-
-"""
-
-# revision identifiers, used by Alembic.
-revision = '2b278a305caf'
-down_revision = '31a259340697'
-
-from alembic import op
-import sqlalchemy as sa
-
-
-def upgrade():
-    ### commands auto generated by Alembic - please adjust! ###
-    op.add_column('commit', sa.Column('committer_email', sa.String(), nullable=True))
-    op.add_column('commit', sa.Column('committer_name', sa.String(), nullable=True))
-    ### end Alembic commands ###
-
-
-def downgrade():
-    ### commands auto generated by Alembic - please adjust! ###
-    op.drop_column('commit', 'committer_name')
-    op.drop_column('commit', 'committer_email')
-    ### end Alembic commands ###
diff --git a/ci/platform/alembic/versions/31a259340697_add_github_repo_meta.py b/ci/platform/alembic/versions/31a259340697_add_github_repo_meta.py
deleted file mode 100644
index e3d4ce4..0000000
--- a/ci/platform/alembic/versions/31a259340697_add_github_repo_meta.py
+++ /dev/null
@@ -1,38 +0,0 @@
-"""Add github repo metadata table
-
-Revision ID: 31a259340697
-Revises: e689879ce67
-Create Date: 2013-08-30 19:03:08.962353
-
-"""
-
-# revision identifiers, used by Alembic.
-revision = '31a259340697'
-down_revision = 'e689879ce67'
-
-from alembic import op
-import sqlalchemy as sa
-
-
-def upgrade():
-    ### commands auto generated by Alembic - please adjust! ###
-    op.create_table('github_repo_metadata',
-    sa.Column('id', sa.Integer(), nullable=False),
-    sa.Column('repo_id', sa.Integer(), nullable=False),
-    sa.Column('owner_name', sa.String(), nullable=False),
-    sa.Column('repo_name', sa.String(), nullable=False),
-    sa.Column('hook_id', sa.Integer(), nullable=True),
-    sa.Column('hook_secret', sa.String(), nullable=True),
-    sa.Column('added_by_user_id', sa.Integer(), nullable=False),
-    sa.ForeignKeyConstraint(['repo_id'], ['repo.id'], ),
-    sa.ForeignKeyConstraint(['added_by_user_id'], ['user.id'], ),
-    sa.PrimaryKeyConstraint('id'),
-    sa.UniqueConstraint('repo_id')
-    )
-    ### end Alembic commands ###
-
-
-def downgrade():
-    ### commands auto generated by Alembic - please adjust! ###
-    op.drop_table('github_repo_metadata')
-    ### end Alembic commands ###
diff --git a/ci/platform/bunnyrpc/client.py b/ci/platform/bunnyrpc/client.py
index bf04f0d..e4c2027 100644
--- a/ci/platform/bunnyrpc/client.py
+++ b/ci/platform/bunnyrpc/client.py
@@ -148,14 +148,14 @@ class Client(ClientBase):
 			exc_tuple = (proto["error"]["type"],
 						proto["error"]["message"],
 						proto["error"]["traceback"])
-			eval_str = '%s("""%s\n RemoteTraceback (most recent call last):%s""")' % exc_tuple
+			eval_str = "%s(r''' %s\n RemoteTraceback (most recent call last):%s ''')" % exc_tuple
 			try:
 				raise eval(eval_str, self.caller_globals_dict)
 			except:
 				new_exc_tuple = sys.exc_info()
-				if str(new_exc_tuple[0].__name__) == str(exc_tuple[0]):  # If we receive the exception we wanted, everything is good
+				if new_exc_tuple[0].__name__ == exc_tuple[0]:  # If we receive the exception we wanted, everything is good
 					raise
-				raise RPCRequestError(msg=eval_str, original_type=proto["error"]["type"])  # Otherwise, we don't know how to recreate it, so wrap the info
+				raise RPCRequestError(msg=eval_str)  # Otherwise, we don't know how to recreate it, so wrap the info
 		else:
 			return proto["value"]
 
diff --git a/ci/platform/bunnyrpc/exceptions.py b/ci/platform/bunnyrpc/exceptions.py
index 2ead79c..6f483a2 100644
--- a/ci/platform/bunnyrpc/exceptions.py
+++ b/ci/platform/bunnyrpc/exceptions.py
@@ -7,6 +7,5 @@ class RPCError(Exception):
 
 
 class RPCRequestError(RPCError):
-	def __init__(self, msg='', original_type=None):
-		self.original_type = original_type
+	def __init__(self, msg=''):
 		super(RPCRequestError, self).__init__(msg)
diff --git a/ci/platform/conf/redis/platform_redis.conf b/ci/platform/conf/redis/platform_redis.conf
index 9587c6e..f79e28f 100644
--- a/ci/platform/conf/redis/platform_redis.conf
+++ b/ci/platform/conf/redis/platform_redis.conf
@@ -124,7 +124,7 @@ rdbcompression yes
 rdbchecksum yes
 
 # The filename where to dump the DB
-dbfilename platform_redis.rdb
+dbfilename filesystem_repo_server_redis.rdb
 
 # The working directory.
 #
diff --git a/ci/platform/database/schema.py b/ci/platform/database/schema.py
index efbf483..8c9100b 100644
--- a/ci/platform/database/schema.py
+++ b/ci/platform/database/schema.py
@@ -56,9 +56,7 @@ commit = Table('commit', metadata,
 	Column('sha', String, nullable=False),
 	Column('base_sha', String, nullable=True),
 	Column('message', String, nullable=False),
-	Column('timestamp', Integer, nullable=False),
-	Column('committer_name', String, nullable=True),
-	Column('committer_email', String, nullable=True)
+	Column('timestamp', Integer, nullable=False)
 )
 
 change = Table('change', metadata,
@@ -144,16 +142,6 @@ xunit = Table('xunit', metadata,
 	UniqueConstraint('build_console_id', 'path')
 )
 
-github_repo_metadata = Table('github_repo_metadata', metadata,
-	Column('id', Integer, primary_key=True),
-	Column('repo_id', Integer, ForeignKey('repo.id'), nullable=False, unique=True),
-	Column('owner_name', String, nullable=False),
-	Column('repo_name', String, nullable=False),
-	Column('hook_id', Integer, nullable=True),
-	Column('hook_secret', String, nullable=True),
-	Column('added_by_user_id', Integer, ForeignKey('user.id'), nullable=False)
-)
-
 repostore = Table('repostore', metadata,
 	Column('id', Integer, primary_key=True),
 	Column('ip_address', String, nullable=False),
diff --git a/ci/platform/model_server/build_consoles/read_handler.py b/ci/platform/model_server/build_consoles/read_handler.py
index 085a1e0..0539d8c 100644
--- a/ci/platform/model_server/build_consoles/read_handler.py
+++ b/ci/platform/model_server/build_consoles/read_handler.py
@@ -1,6 +1,6 @@
 import database.schema
 
-from sqlalchemy import and_, func, select
+from sqlalchemy import and_
 
 from database.engine import ConnectionFactory
 from model_server.rpc_handler import ModelServerRpcHandler
@@ -47,25 +47,15 @@ class BuildConsolesReadHandler(ModelServerRpcHandler):
 		console_output = database.schema.console_output
 		build_console = database.schema.build_console
 
+		output_query = console_output.select().where(
+			console_output.c.build_console_id == build_console_id
+		).order_by(
+			console_output.c.line_number.desc(),
+			console_output.c.id.asc()
+		).limit(num_results).offset(offset)
+
 		with ConnectionFactory.get_sql_connection() as sqlconn:
-			max_line_query = select([func.max(console_output.c.line_number)]).where(console_output.c.build_console_id == build_console_id)
-			max_line_row = sqlconn.execute(max_line_query).first()
-			if max_line_row and max_line_row[0] is not None:
-				max_line = max_line_row[0]
-				output_query = console_output.select().where(
-					console_output.c.build_console_id == build_console_id
-				).order_by(
-					console_output.c.line_number.desc(),
-					console_output.c.id.asc()
-				).where(
-					and_(
-						console_output.c.line_number > max_line - offset - num_results,
-						console_output.c.line_number <= max_line - offset
-					)
-				)
-				return {row[console_output.c.line_number]: row[console_output.c.line] for row in sqlconn.execute(output_query)}
-			else:
-				return {}
+			return {row[console_output.c.line_number]: row[console_output.c.line] for row in sqlconn.execute(output_query)}
 
 	def get_build_console_id(self, user_id, build_id, type, subtype):
 		build_console = database.schema.build_console
diff --git a/ci/platform/model_server/changes/create_handler.py b/ci/platform/model_server/changes/create_handler.py
index 090867c..827f8c7 100644
--- a/ci/platform/model_server/changes/create_handler.py
+++ b/ci/platform/model_server/changes/create_handler.py
@@ -3,7 +3,6 @@ import time
 import database.schema
 import repo.store as repostore
 
-from sqlalchemy import and_
 from shared.constants import BuildStatus
 from database.engine import ConnectionFactory
 from model_server.rpc_handler import ModelServerRpcHandler
@@ -12,7 +11,6 @@ from sqlalchemy.sql import func
 from util import pathgen
 from util.log import Logged
 from util.sql import to_dict
-from repo.store import DistributedLoadBalancingRemoteRepositoryManager
 
 # Debug instance default timeout is 50 minutes (less than one hour with boot)
 DEFAULT_TIMEOUT = 50*60
@@ -22,28 +20,14 @@ class ChangesCreateHandler(ModelServerRpcHandler):
 	def __init__(self, channel=None):
 		super(ChangesCreateHandler, self).__init__("changes", "create", channel)
 
-	def create_commit_and_change(self, repo_id, user_id, base_sha, head_sha, merge_target, verify_only=False, patch_contents=None):
-		repo_id = int(repo_id)
-		user_id = int(user_id)
+	def create_commit_and_change(self, repo_id, user_id, commit_message, sha, merge_target, base_sha, store_pending=False, patch_contents=None):
+		commit_id = self._create_commit(repo_id, user_id, commit_message, sha, base_sha, store_pending)
 
 		change = database.schema.change
 		repo = database.schema.repo
 		user = database.schema.user
 		commit = database.schema.commit
 
-		remote_repo_manager = DistributedLoadBalancingRemoteRepositoryManager(ConnectionFactory.get_redis_connection('repostore'))
-
-		with ConnectionFactory.get_sql_connection() as sqlconn:
-			repo_type_query = repo.select().where(repo.c.id == repo_id)
-			repo_row = sqlconn.execute(repo_type_query).first()
-			repo_type = repo_row[repo.c.type]
-			repostore_id = repo_row[repo.c.repostore_id]
-			repo_name = repo_row[repo.c.name]
-
-		commit_attributes = remote_repo_manager.get_commit_attributes(repostore_id, repo_id, repo_name, head_sha)
-
-		commit_id = self._create_commit(repo_id, user_id, commit_attributes, base_sha, head_sha, verify_only)
-
 		prev_change_number = 0
 
 		create_time = int(time.time())
@@ -58,7 +42,9 @@ class ChangesCreateHandler(ModelServerRpcHandler):
 				number=change_number, verification_status=BuildStatus.QUEUED, create_time=create_time)
 			result = sqlconn.execute(ins)
 			change_id = result.inserted_primary_key[0]
-
+			repo_type_query = repo.select().where(repo.c.id == repo_id)
+			repo_row = sqlconn.execute(repo_type_query).first()
+			repo_type = repo_row[repo.c.type]
 
 			query = user.select().where(user.c.id == user_id)
 			user_row = sqlconn.execute(query).first()
@@ -71,42 +57,11 @@ class ChangesCreateHandler(ModelServerRpcHandler):
 		commit_dict = to_dict(commit_row, commit.columns)
 		patch_id = self.store_patch(change_id, patch_contents) if patch_contents else None
 
-		if repo_type == 'hg':
-			merge_target = commit_attributes['branch']
-
-		skip = False
-
-		if '[ci skip]' in commit_attributes['message']:
-			skip = True
-		elif '[ci test_only]' in commit_attributes['message']:
-			verify_only = True
-
 		self.publish_event("repos", repo_id, "change added", user=user_dict, commit=commit_dict,
 			repo_type=repo_type, change_id=change_id, change_number=change_number, verification_status="queued",
-			merge_target=merge_target, create_time=create_time, patch_id=patch_id, verify_only=verify_only, skip=skip)
+			merge_target=merge_target, create_time=create_time, patch_id=patch_id)
 		return {"change_id": change_id, "commit_id": commit_id}
 
-	def create_github_commit_and_change(self, user_id, github_owner_name, github_repo_name, base_sha, head_sha, branch_name):
-		github_repo_metadata = database.schema.github_repo_metadata
-		repo = database.schema.repo
-
-		with ConnectionFactory.get_sql_connection() as sqlconn:
-			query = github_repo_metadata.join(repo).select().apply_labels().where(
-				and_(
-					repo.c.deleted == 0,
-					github_repo_metadata.c.repo_name == github_repo_name,
-					github_repo_metadata.c.owner_name == github_owner_name,
-				)
-			)
-			row = sqlconn.execute(query).first()
-			if row is not None:
-				repo_id = row[repo.c.id]
-			else:
-				raise RepositoryNotFoundError(github_repo_name, github_owner_name)
-
-		verify_only = True
-		return self.create_commit_and_change(repo_id, user_id, base_sha, head_sha, branch_name, verify_only)
-
 	def launch_debug_instance(self, user_id, change_id, timeout=DEFAULT_TIMEOUT):
 		if not isinstance(timeout, (int, float)) or timeout < 0:
 			timeout = DEFAULT_TIMEOUT
@@ -121,21 +76,20 @@ class ChangesCreateHandler(ModelServerRpcHandler):
 			patch_id = result.inserted_primary_key[0]
 		return patch_id
 
-	def _create_commit(self, repo_id, user_id, commit_attributes, base_sha, head_sha, verify_only):
+	def _create_commit(self, repo_id, user_id, commit_message, sha, base_sha, store_pending):
 		commit = database.schema.commit
 
 		timestamp = int(time.time())
 		ins = commit.insert().values(repo_id=repo_id, user_id=user_id,
-			message=commit_attributes['message'], sha=head_sha, base_sha=base_sha, timestamp=timestamp,
-			committer_name=commit_attributes['username'], committer_email=commit_attributes['email'])
+			message=commit_message, sha=sha, base_sha=base_sha, timestamp=timestamp)
 		with ConnectionFactory.get_sql_connection() as sqlconn:
 			result = sqlconn.execute(ins)
 		commit_id = result.inserted_primary_key[0]
 
-		if verify_only:
-			self._store_pending_commit(repo_id, head_sha, commit_id)
+		if store_pending:
+			self._store_pending_commit(repo_id, sha, commit_id)
 
-		self._push_pending_commit(repo_id, head_sha, commit_id)
+		self._push_pending_commit(repo_id, sha, commit_id)
 
 		return commit_id
 
@@ -166,7 +120,3 @@ class ChangesCreateHandler(ModelServerRpcHandler):
 
 class NoSuchCommitError(Exception):
 	pass
-
-
-class RepositoryNotFoundError(Exception):
-	pass
diff --git a/ci/platform/model_server/debug_instances/update_handler.py b/ci/platform/model_server/debug_instances/update_handler.py
index 8d20fcb..a554d5d 100644
--- a/ci/platform/model_server/debug_instances/update_handler.py
+++ b/ci/platform/model_server/debug_instances/update_handler.py
@@ -11,7 +11,7 @@ Your debug instance has launched and will be accessible for the next 50 minutes.
 
 To SSH into your debug instance type the following command into your terminal:
 
-ssh verification@%s -Ct "ssh %s"
+ssh verification@%s -t "ssh %s"
 
 -The Koality Team"""
 
diff --git a/ci/platform/model_server/repos/create_handler.py b/ci/platform/model_server/repos/create_handler.py
index 45f4187..b172750 100644
--- a/ci/platform/model_server/repos/create_handler.py
+++ b/ci/platform/model_server/repos/create_handler.py
@@ -23,36 +23,6 @@ class ReposCreateHandler(ModelServerRpcHandler):
 
 	@AdminApi
 	def create_repo(self, user_id, repo_name, forward_url, repo_type):
-		repo_id, current_time = self._create_repo(user_id, repo_name, forward_url, repo_type)
-
-		self.publish_event_to_all("users", "repository added", repo_id=repo_id, repo_name=repo_name, forward_url=forward_url, created=current_time)
-		return repo_id
-
-	@AdminApi
-	def create_github_repo(self, user_id, repo_name, github_owner_name, github_repo_name, forward_url):
-		github_repo_metadata = database.schema.github_repo_metadata
-
-		repo_id, current_time = self._create_repo(user_id, repo_name, forward_url, 'git')
-
-		insert_github_metadata = github_repo_metadata.insert().values(
-			repo_id=repo_id,
-			owner_name=github_owner_name,
-			repo_name=github_repo_name,
-			added_by_user_id=user_id
-		)
-		with ConnectionFactory.get_sql_connection() as sqlconn:
-			sqlconn.execute(insert_github_metadata)
-
-		github_object = {
-			'owner_name': github_owner_name,
-			'repo_name': github_repo_name
-		}
-
-		self.publish_event_to_all("users", "repository added", repo_id=repo_id, repo_name=repo_name, forward_url=forward_url, created=current_time, github=github_object)
-
-		return repo_id
-
-	def _create_repo(self, user_id, repo_name, forward_url, repo_type):
 		if not repo_name:
 			raise RepositoryCreateError("repo_name cannot be empty")
 		elif re.match('^[-_a-zA-Z0-9]+$', repo_name) is None:
@@ -86,8 +56,9 @@ class ReposCreateHandler(ModelServerRpcHandler):
 				repo_type)
 			# make filesystem changes
 			self._create_repo_on_filesystem(manager, repostore_id, repo_id, repo_name)
-			return repo_id, current_time
 
+			self.publish_event_to_all("users", "repository added", repo_id=repo_id, repo_name=repo_name, forward_url=forward_url, created=current_time)
+			return repo_id
 		except repo.store.BadRepositorySetupError as e:
 			with model_server.rpc_connect('repos', 'delete') as repos_delete_handler:
 				repos_delete_handler.delete_repo(user_id, repo_id)
diff --git a/ci/platform/model_server/repos/read_handler.py b/ci/platform/model_server/repos/read_handler.py
index 6c15268..9d08692 100644
--- a/ci/platform/model_server/repos/read_handler.py
+++ b/ci/platform/model_server/repos/read_handler.py
@@ -96,45 +96,23 @@ class ReposReadHandler(ModelServerRpcHandler):
 	# Front end API #
 	#################
 
-	def _row_to_repo(self, row):
-		repo = database.schema.repo
-		github_repo_metadata = database.schema.github_repo_metadata
-
-		repo_info = to_dict(row, repo.columns, tablename=repo.name)
-		if row[github_repo_metadata.c.id] is not None:
-			repo_info['github'] = to_dict(row, github_repo_metadata.columns, tablename=github_repo_metadata.name)
-
-			user = database.schema.user
-			query = user.select().where(user.c.id == repo_info['github']['added_by_user_id'])
-			with ConnectionFactory.get_sql_connection() as sqlconn:
-				user_row = sqlconn.execute(query).first()
-
-			repo_info['github']['added_by_user'] = to_dict(user_row, user.columns)
-		else:
-			repo_info['github'] = None
-		return repo_info
-
-
 	def get_repositories(self, user_id):
 		repo = database.schema.repo
-		github_repo_metadata = database.schema.github_repo_metadata
 
-		query = repo.outerjoin(github_repo_metadata).select().apply_labels().where(repo.c.deleted == 0)  # Check to make sure its not deleted
+		query = repo.select().apply_labels().where(repo.c.deleted == 0)  # Check to make sure its not deleted
 		with ConnectionFactory.get_sql_connection() as sqlconn:
 			rows = sqlconn.execute(query)
-
-		return map(self._row_to_repo, rows)
+		return map(lambda row: to_dict(row, repo.columns, tablename=repo.name), rows)
 
 	def get_repo_from_id(self, user_id, repo_id):
 		repo = database.schema.repo
-		github_repo_metadata = database.schema.github_repo_metadata
 
-		query = repo.outerjoin(github_repo_metadata).select().apply_labels().where(and_(repo.c.id == repo_id, repo.c.deleted == 0))
+		query = repo.select().where(and_(repo.c.id == repo_id, repo.c.deleted == 0))
 		with ConnectionFactory.get_sql_connection() as sqlconn:
 			row = sqlconn.execute(query).first()
 
 		if row:
-			return self._row_to_repo(row)
+			return to_dict(row, repo.columns)
 		else:
 			raise NoSuchRepositoryError(repo_id)
 
diff --git a/ci/platform/model_server/repos/update_handler.py b/ci/platform/model_server/repos/update_handler.py
index c2a9c3a..699435a 100644
--- a/ci/platform/model_server/repos/update_handler.py
+++ b/ci/platform/model_server/repos/update_handler.py
@@ -3,13 +3,12 @@ import repo.store
 
 from database.engine import ConnectionFactory
 from model_server.rpc_handler import ModelServerRpcHandler
-from util.permissions import AdminApi
 
 
 class ReposUpdateHandler(ModelServerRpcHandler):
 
 	def __init__(self, channel=None):
-		super(ReposUpdateHandler, self).__init__('repos', 'update', channel)
+		super(ReposUpdateHandler, self).__init__("repos", "update", channel)
 
 	def update_repostore(self, repostore_id, ip_address, root_dir, num_repos):
 		self.update_repostore_ip(repostore_id, ip_address)
@@ -46,7 +45,6 @@ class ReposUpdateHandler(ModelServerRpcHandler):
 		manager = repo.store.DistributedLoadBalancingRemoteRepositoryManager(ConnectionFactory.get_redis_connection('repostore'))
 		return manager.force_delete(info['repostore_id'], repo_id, info['repo_name'], target)
 
-	@AdminApi
 	def set_forward_url(self, user_id, repo_id, forward_url):
 		repo = database.schema.repo
 
@@ -54,16 +52,7 @@ class ReposUpdateHandler(ModelServerRpcHandler):
 		update = repo.update().where(repo.c.id == repo_id).values(forward_url=forward_url)
 		with ConnectionFactory.get_sql_connection() as sqlconn:
 			sqlconn.execute(update)
-		self.publish_event('repos', repo_id, 'forward url updated', forward_url=forward_url)
-
-	@AdminApi
-	def set_github_hook(self, user_id, repo_id, hook_id, hook_secret):
-		github_repo_metadata = database.schema.github_repo_metadata
-
-		update = github_repo_metadata.update().where(github_repo_metadata.c.repo_id == repo_id).values(hook_id=hook_id, hook_secret=hook_secret)
-		with ConnectionFactory.get_sql_connection() as sqlconn:
-			sqlconn.execute(update)
-		self.publish_event('repos', repo_id, 'github hook added', hook_id=hook_id, hook_secret=hook_secret)
+		self.publish_event("repos", repo_id, "forward url updated", forward_url=forward_url)
 
 
 class NoSuchUserError(Exception):
diff --git a/ci/platform/repo/store.py b/ci/platform/repo/store.py
index 37afea0..e982a45 100644
--- a/ci/platform/repo/store.py
+++ b/ci/platform/repo/store.py
@@ -20,38 +20,94 @@ from hglib.error import CommandError
 
 from bunnyrpc.client import Client
 from settings.store import StoreSettings
+from util import greenlets, pathgen
 from util.log import Logged
-from util import pathgen
 
 MINUTE = 60
 
 
 class RemoteRepositoryManager(object):
 	def register_remote_store(self, repostore_id, num_repos):
+		"""Registers a remote store as a managed store of this manager
+
+		:param repostore_id: The identifier of the remote store the repository is on
+		:param num_repos: The number of repos to initialize it with
+		"""
 		raise NotImplementedError("Subclasses should override this!")
 
 	def merge_changeset(self, repostore_id, repo_id, repo_name, ref_to_merge, ref_to_merge_into):
+		"""Merges a changeset on the remote repository with a ref.
+
+		:param repostore_id: The identifier of the local store(machine) the
+							repository is on.
+		:param repo_id: The unique identifier for the repository being created.
+		:param repo_name: The name of the repository.
+		:param ref_to_merge: The sha ref of the changeset on the remote
+							repository we want to merge.
+		:param ref_to_merge_into: The ref we want to merge into.
+		"""
 		raise NotImplementedError("Subclasses should override this!")
 
 	def create_repository(self, repostore_id, repo_id, repo_name):
+		"""Creates a repository on the given local store.
+
+		:param repostore_id: The identifier of the local store(machine) to create the repository on.
+		:param repo_id: The unique identifier for the repository being created.
+		:param repo_name: The name of the new repository.
+		"""
 		raise NotImplementedError("Subclasses should override this!")
 
 	def delete_repository(self, repostore_id, repo_id, repo_name):
+		"""Deletes a repository on the given local store.
+
+		:param repostore_id: The identifier of the local store(machine) to create the repository on.
+		:param repo_id: The unique identifier for the repository being created.
+		:param repo_name: The name of the new repository.
+		"""
 		raise NotImplementedError("Subclasses should override this!")
 
 	def push(self, repostore_id, repo_id, repo_name, from_target, to_target, force):
+		"""Pushes the repository to the forwarding url
+
+		:param repostore_id: The identifier of the local machine the repo is on
+		:param repo_id: The unique id of the RepositoryStore
+		:param repo_name: The name of the repo
+		:param from_target: The ref we're pushing to target
+		:param to_target: The ref to push
+		:param force: Whether or not to force push
+		"""
 		raise NotImplementedError("Subclasses should override this!")
 
 	def force_delete(self, repostore_id, repo_id, repo_name, target):
+		"""Force pushes the repository to the forwarding url
+
+		:param repostore_id: The identifier of the local machine the repo is on
+		:param repo_id: The unique id of the RepositoryStore
+		:param repo_name: The name of the repo
+		:param target: The ref to delete
+		"""
 		raise NotImplementedError("Subclasses should override this!")
 
 	def store_pending(self, repostore_id, repo_id, repo_name, sha, commit_id):
+		"""Stores the given sha as a pending ref based on the commit id
+
+		:param repostore_id: The identifier of the local machine the repo is on
+		:param repo_id: The unique id of the RepositoryStore
+		:param repo_name: The name of the repo
+		:param sha: The SHA-1 ref of the commit to store
+		:param commit_id: The pending id to store the SHA-1 ref at
+		"""
+
 		raise NotImplementedError("Subclasses should override this!")
 
 	def rename_repository(self, repostore_id, repo_id, old_repo_name, new_repo_name):
-		raise NotImplementedError("Subclasses should override this!")
+		"""Renames a repository on the given local store.
 
-	def get_commit_attributes(self, repostore_id, repo_id, repo_name, sha):
+		:param repostore_id: The identifier of the local store(machine) to create the repository on.
+		:param repo_id: The unique identifier for the repository being created.
+		:param old_repo_name: The name of the old repository.
+		:param new_repo_name: The name of the new repository.
+		"""
 		raise NotImplementedError("Subclasses should override this!")
 
 
@@ -88,20 +144,14 @@ class DistributedLoadBalancingRemoteRepositoryManager(RemoteRepositoryManager):
 		self._update_store_repo_count(repostore_id, -1)
 
 	def push(self, repostore_id, repo_id, repo_name, from_target, to_target, force):
-		assert isinstance(repo_id, int)
-
 		with Client(StoreSettings.rpc_exchange_name, RepositoryStore.queue_name(repostore_id), globals=globals()) as client:
 			client.push(repo_id, repo_name, from_target, to_target, force)
 
 	def force_delete(self, repostore_id, repo_id, repo_name, target):
-		assert isinstance(repo_id, int)
-
 		with Client(StoreSettings.rpc_exchange_name, RepositoryStore.queue_name(repostore_id), globals=globals()) as client:
 			return client.force_delete(repo_id, repo_name, target)
 
 	def store_pending(self, repostore_id, repo_id, repo_name, sha, commit_id):
-		assert isinstance(repo_id, int)
-
 		with Client(StoreSettings.rpc_exchange_name, RepositoryStore.queue_name(repostore_id), globals=globals()) as client:
 			return client.store_pending(repo_id, repo_name, sha, commit_id)
 
@@ -111,12 +161,6 @@ class DistributedLoadBalancingRemoteRepositoryManager(RemoteRepositoryManager):
 		with Client(StoreSettings.rpc_exchange_name, RepositoryStore.queue_name(repostore_id), globals=globals()) as client:
 			client.rename_repository(repo_id, old_repo_name, new_repo_name)
 
-	def get_commit_attributes(self, repostore_id, repo_id, repo_name, sha):
-		assert isinstance(repo_id, int)
-
-		with Client(StoreSettings.rpc_exchange_name, RepositoryStore.queue_name(repostore_id), globals=globals()) as client:
-			return client.get_commit_attributes(repo_id, repo_name, sha)
-
 	def get_least_loaded_store(self):
 		"""Identifies the local store that is being least utilized. For this particular class
 		least utilized is defined as having the lowest number of repositories."""
@@ -205,9 +249,6 @@ class RepositoryStore(object):
 	def rename_repository(self, repo_id, old_repo_name, new_repo_name):
 		raise NotImplementedError("Subclasses should override this!")
 
-	def get_commit_attributes(self, repo_id, repo_name, sha):
-		raise NotImplementedError("Subclasses should override this!")
-
 
 @Logged()
 class FileSystemRepositoryStore(RepositoryStore):
@@ -314,7 +355,7 @@ class FileSystemRepositoryStore(RepositoryStore):
 			try:
 				self._hg_fetch_with_private_key(repo, remote_repo)
 				repo.merge(tool="internal:fail")
-				rev, sha = repo.commit("Merging in %s" % sha)
+				rev, sha = repo.commit("Merging in %s" % sha[:12])
 				return sha
 			except CommandError:
 				exc_info = sys.exc_info()
@@ -418,11 +459,12 @@ class FileSystemRepositoryStore(RepositoryStore):
 
 		elif repo_type == "hg":
 			repo_path = self._resolve_path(repo_id, repo_name)
-			with hglib.open(repo_path) as repo:
-				self._hg_fetch_with_private_key(repo, remote_repo)
-				# The rev argument is to make sure that we only pull the revision and it's dependencies into the repository.
-				repo.pull(os.path.join(repo_path, ".hg", "strip-backup", ref_to_merge + ".hg"), rev=ref_to_merge, update=True)
-				self._hg_push_merge_retry(repo, remote_repo, ref_to_merge, ref_to_merge_into)
+			repo = hglib.open(repo_path)
+
+			self._hg_fetch_with_private_key(repo, remote_repo)
+			# The rev argument is to make sure that we only pull the revision and it's dependencies into the repository.
+			repo.pull(os.path.join(repo_path, ".hg", "strip-backup", ref_to_merge + ".hg"), rev=ref_to_merge, update=True)
+			self._hg_push_merge_retry(repo, remote_repo, ref_to_merge, ref_to_merge_into)
 		else:
 			return
 
@@ -455,7 +497,7 @@ class FileSystemRepositoryStore(RepositoryStore):
 			repo = Repo.init(repo_path, bare=True)
 			try:
 				# The explicit refspec pulls down all heads and sets them as the local heads
-				self._git_fetch_with_private_key(repo, remote_repo, '+refs/heads/*:refs/heads/*')
+				self._git_fetch_with_private_key(repo, remote_repo, 'refs/heads/*:refs/heads/*')
 			except GitCommandError:
 				error_msg = "Pull failed for repo with id %s and forward url %s" % (repo_id, remote_repo)
 				self.logger.warn(error_msg, exc_info=True)
@@ -466,8 +508,7 @@ class FileSystemRepositoryStore(RepositoryStore):
 			make_repo_dirs()
 			hglib.init(repo_path)
 			try:
-				with hglib.open(repo_path) as repo:
-					self._hg_fetch_with_private_key(repo, remote_repo)
+				self._hg_fetch_with_private_key(hglib.open(repo_path), remote_repo)
 			except CommandError:
 				error_msg = "Pull failed for repo with id %s and forward url %s" % (repo_id, remote_repo)
 				self.logger.warn(error_msg, exc_info=True)
@@ -590,12 +631,12 @@ class FileSystemRepositoryStore(RepositoryStore):
 		with model_server.rpc_connect("repos", "read") as conn:
 			remote_repo = conn.get_repo_forward_url(repo_id)
 
-		self._git_fetch_with_private_key(repo, remote_repo, '+refs/heads/*:refs/heads/*')
+		self._git_fetch_with_private_key(repo, remote_repo)
 
 		try:
 			repo.commit(sha)
 		except git.exc.BadObject:
-			raise NoSuchCommitError(repo_id=repo_id, ref=sha)
+			raise NoSuchCommitError(repo_id, sha)
 
 		try:
 			refs.SymbolicReference.create(repo, 'refs/pending/%d' % commit_id, sha)
@@ -606,28 +647,28 @@ class FileSystemRepositoryStore(RepositoryStore):
 
 	def _hg_store_pending(self, repo_id, repo_name, sha, commit_id):
 		repo_path = self._resolve_path(repo_id, repo_name)
+		repo = hglib.open(repo_path)
 
-		with hglib.open(repo_path) as repo:
-			with model_server.rpc_connect("repos", "read") as conn:
-				remote_repo = conn.get_repo_forward_url(repo_id)
+		with model_server.rpc_connect("repos", "read") as conn:
+			remote_repo = conn.get_repo_forward_url(repo_id)
 
-			self._hg_fetch_with_private_key(repo, remote_repo)
+		self._hg_fetch_with_private_key(repo, remote_repo)
 
-			try:
-				repo.update(sha)
-			except CommandError:
-				raise NoSuchCommitError(repo_id=repo_id, ref=sha)
+		try:
+			repo.update(sha)
+		except CommandError:
+			raise NoSuchCommitError(repo_id, sha)
 
-			try:
-				parent_shas = map(lambda rev: rev.node, repo.log(['parents(%s)' % sha]))
-				strip_path = os.path.join(repo_path, ".hg", "strip-backup")
-				if not os.path.exists(strip_path):
-					os.makedirs(os.path.join(strip_path))
-				repo.bundle(os.path.join(strip_path, '%s.hg' % sha), rev=[sha], base=parent_shas)
-			except:
-				exc_info = sys.exc_info()
-				self.logger.critical("Failed to create pending bundle %d for sha %s" % (commit_id, sha), exc_info=exc_info)
-				raise exc_info
+		try:
+			parent_shas = map(lambda rev: rev.node, repo.log(['parents(%s)' % sha]))
+			strip_path = os.path.join(repo_path, ".hg", "strip-backup")
+			if not os.path.exists(strip_path):
+				os.makedirs(os.path.join(strip_path))
+			repo.bundle(os.path.join(strip_path, '%s.hg' % sha[:12]), rev=[sha], base=parent_shas)
+		except:
+			exc_info = sys.exc_info()
+			self.logger.critical("Failed to create pending bundle %d for sha %s" % (commit_id, sha), exc_info=exc_info)
+			raise exc_info
 
 	def rename_repository(self, repo_id, old_name, new_name):
 		"""Renames a repository. Raises an exception on failure.
@@ -650,55 +691,6 @@ class FileSystemRepositoryStore(RepositoryStore):
 			shutil.move(old_repo_path, new_repo_path)
 		else:
 			raise RepositoryAlreadyExistsException(repo_id, new_repo_path)
-	# TODO (akostov) unify with overlapping parts of store-pending
-	def get_commit_attributes(self, repo_id, repo_name, sha):
-		assert isinstance(repo_id, int)
-		commit_attributes = dict()
-
-		repo_type = self._get_repo_type(repo_id)
-
-		with model_server.rpc_connect("repos", "read") as conn:
-				remote_repo = conn.get_repo_forward_url(repo_id)
-
-		if repo_type == "git":
-			repo_name += '.git'
-			repo_path = self._resolve_path(repo_id, repo_name)
-			repo = Repo(repo_path)
-
-			self._git_fetch_with_private_key(repo, remote_repo, '+refs/heads/*:refs/heads/*')
-
-			try:
-				commit = repo.commit(sha)
-			except git.exc.BadObject:
-				raise NoSuchCommitError(repo_id, sha)
-
-			commit_attributes["message"] = commit.message
-			commit_attributes["username"] = commit.author.name
-			commit_attributes["email"] = commit.author.email
-
-		elif repo_type == "hg":
-			repo_path = self._resolve_path(repo_id, repo_name)
-			bundle_path = os.path.join(repo_path, ".hg", "strip-backup", sha + ".hg")
-
-			if os.path.exists(bundle_path):
-				with hglib.open('bundle:%s+%s' % (repo_path, bundle_path)) as repo:
-					log = repo.log(sha)[0]
-			else:
-				with hglib.open(repo_path) as repo:
-					self._hg_fetch_with_private_key(repo, remote_repo)
-					try:
-						log = repo.log(sha)[0]
-					except CommandError:
-						raise NoSuchCommitError(repo_id, sha)
-
-			commit_attributes["message"] = log[5]
-			commit_attributes["username"] = log[4].split('<')[0].strip()
-			commit_attributes["email"] = log[4].split('<')[1].strip('> ')
-			commit_attributes["branch"] = log[3]
-		else:
-			return
-
-		return commit_attributes
 
 	def _resolve_path(self, repo_id, repo_name):
 		repo_path = os.path.join(self._root_path, pathgen.to_path(repo_id, repo_name))
@@ -745,7 +737,6 @@ class RepositoryAlreadyExistsException(RepositoryOperationException):
 class NoSuchCommitError(RepositoryOperationException):
 	"""Indicates an exception occured trying to dereference a given ref."""
 
-	def __init__(self, msg='', repo_id=None, ref=None):
-		if not msg:
-			msg = 'Could not find commit %s for repo %d' % (ref, repo_id)
+	def __init__(self, repo_id, ref):
+		msg = 'Could not find commit %s for repo %d' % (ref, repo_id)
 		super(RepositoryOperationException, self).__init__(msg)
diff --git a/ci/platform/requirements.txt b/ci/platform/requirements.txt
index d758377..e9c7ae5 100644
--- a/ci/platform/requirements.txt
+++ b/ci/platform/requirements.txt
@@ -16,8 +16,5 @@ boto==2.6.0
 alembic==0.6.0
 simplejson==3.1.3
 python-hglib==1.0
-paramiko==1.11.0
 git+ssh://git@github.com/LessThanThreeLabs/libcloud.git#egg=apache-libcloud
-git+ssh://git@github.com/LessThanThreeLabs/pysh.git@0.1.0
-git+ssh://git@github.com/LessThanThreeLabs/koality-streaming-executor.git@0.4.1
-git+ssh://git@github.com/LessThanThreeLabs/koality-provisioner.git@0.4.1
+git+ssh://git@github.com/LessThanThreeLabs/koality-streaming-executor.git@0.3#egg=streaming-executor-0.3
diff --git a/ci/platform/scripts/repo/store_pending_and_trigger_build.py b/ci/platform/scripts/repo/store_pending_and_trigger_build.py
index ce7fcd4..030d673 100755
--- a/ci/platform/scripts/repo/store_pending_and_trigger_build.py
+++ b/ci/platform/scripts/repo/store_pending_and_trigger_build.py
@@ -11,25 +11,25 @@ def main():
 	user_id = int(sys.argv[1])
 	repo_dir = sys.argv[2]
 	repo_id = pathgen.get_repo_id(repo_dir)
-	# TODO(andrey) remove message passing from the custom hg and jgit
-	# message = sys.argv[3]
+	message = sys.argv[3]
 	sha = sys.argv[4]
 	merge_target = sys.argv[5]
 	base_sha = sys.argv[6] if len(sys.argv) == 7 else None
 	if not DeploymentSettings.active:
 		print >> sys.stderr, '\033[33;1m' + 'Koality is currently deactivated.\nYour change will not be verified.' + '\033[0m'
-	pending_change_ref = store_pending_ref_and_trigger_build(user_id, repo_id, base_sha, sha, merge_target)
+	pending_change_ref = store_pending_ref_and_trigger_build(user_id, repo_id, message, sha, merge_target, base_sha)
 	print pending_change_ref
 
 
-def store_pending_ref_and_trigger_build(user_id, repo_id, base_sha, sha, merge_target):
+def store_pending_ref_and_trigger_build(user_id, repo_id, message, sha, merge_target, base_sha):
 	with model_server.rpc_connect("changes", "create") as client:
 		commit_id = client.create_commit_and_change(
 			repo_id,
 			user_id,
-			base_sha,
+			message,
 			sha,
-			merge_target)["commit_id"]
+			merge_target,
+			base_sha)["commit_id"]
 	return pathgen.hidden_ref(commit_id)
 
 if __name__ == '__main__':
diff --git a/ci/platform/scripts/server/start_verification_server.py b/ci/platform/scripts/server/start_verification_server.py
index f968c5c..5e2ffa3 100755
--- a/ci/platform/scripts/server/start_verification_server.py
+++ b/ci/platform/scripts/server/start_verification_server.py
@@ -1,6 +1,4 @@
 #!/usr/bin/env python
-from util import greenlets
-
 import argparse
 import sys
 
diff --git a/ci/platform/scripts/ssh/serve.py b/ci/platform/scripts/ssh/serve.py
index f2567d9..ed16b74 100755
--- a/ci/platform/scripts/ssh/serve.py
+++ b/ci/platform/scripts/ssh/serve.py
@@ -20,9 +20,7 @@ def main():
 	try:
 		if "SSH_ORIGINAL_COMMAND" in os.environ:
 			command = os.environ["SSH_ORIGINAL_COMMAND"] + ' ' + user_id
-			if command.split()[0] == "true":
-				os.execlp('true', 'true')
-			elif command.split()[0] == "ssh":
+			if command.split()[0] == "ssh":
 				rsh = RestrictedSSHForwardingShell()
 			elif command.split()[0] == 'hg':
 				rsh = RestrictedHgShell()
diff --git a/ci/platform/settings/aws.py b/ci/platform/settings/aws.py
index 4c3020e..e868565 100644
--- a/ci/platform/settings/aws.py
+++ b/ci/platform/settings/aws.py
@@ -11,7 +11,7 @@ class AwsSettings(DatabaseBackedSettings):
 			largest_instance_type=None,
 			vm_image_name_prefix='koality_verification',
 			vm_image_name_suffix='precise',
-			vm_image_name_version='0.4',
+			vm_image_name_version='0.3',
 			security_group='koality_verification',
 			root_drive_size=8,
 			s3_bucket_name='')
diff --git a/ci/platform/setup.py b/ci/platform/setup.py
index f728188..8259155 100644
--- a/ci/platform/setup.py
+++ b/ci/platform/setup.py
@@ -1,9 +1,8 @@
 from setuptools import setup, find_packages
-from shared.constants import VERSION
 
 setup(
 	name="koality",
-	version=VERSION,
+	version="0.3",
 	description="Production code for koality",
 	packages=find_packages(exclude=[
 		"bin",
diff --git a/ci/platform/shared/constants.py b/ci/platform/shared/constants.py
index 23b8912..75fd096 100644
--- a/ci/platform/shared/constants.py
+++ b/ci/platform/shared/constants.py
@@ -1,4 +1,4 @@
-VERSION = '0.4.5'
+VERSION = '0.3'
 
 
 class VerificationUser(object):
diff --git a/ci/platform/tests/integration_tests/build_verifier_tests.py b/ci/platform/tests/integration_tests/build_verifier_tests.py
index 14cea03..d6a8bb1 100644
--- a/ci/platform/tests/integration_tests/build_verifier_tests.py
+++ b/ci/platform/tests/integration_tests/build_verifier_tests.py
@@ -39,8 +39,7 @@ class BuildVerifierTest(BaseIntegrationTest, ModelServerTestMixin, RabbitMixin,
 	def _insert_commit_info(self, commit_id, change_id, repo_id):
 		with ConnectionFactory.get_sql_connection() as conn:
 			ins_commit = schema.commit.insert().values(id=commit_id, repo_id=repo_id,
-				user_id=1, message="commit message", sha="sha", timestamp=8675309,
-				committer_name="committer name", committer_email="committer email")
+				user_id=1, message="commit message", sha="sha", timestamp=8675309)
 			conn.execute(ins_commit)
 			ins_change = schema.change.insert().values(id=change_id, commit_id=commit_id, repo_id=repo_id, merge_target="master",
 				number=1, verification_status=BuildStatus.QUEUED, create_time=8675309)
diff --git a/ci/platform/tests/integration_tests/complete_verification_tests.py b/ci/platform/tests/integration_tests/complete_verification_tests.py
index f58eb43..d4b3d1e 100644
--- a/ci/platform/tests/integration_tests/complete_verification_tests.py
+++ b/ci/platform/tests/integration_tests/complete_verification_tests.py
@@ -48,9 +48,9 @@ class VerificationRoundTripTest(BaseIntegrationTest, ModelServerTestMixin, Rabbi
 			super(VerificationRoundTripTest.TestChangeVerifier, self).__init__(verifier_pool, None)
 			self._change_finished = eventlet.event.Event()
 
-		def verify_change(self, verification_config, change_id, repo_type, workers_spawned, verify_only, patch_id=None):
+		def verify_change(self, verification_config, change_id, repo_type, workers_spawned, patch_id=None):
 			try:
-				super(VerificationRoundTripTest.TestChangeVerifier, self).verify_change(verification_config, change_id, repo_type, workers_spawned, False, patch_id)
+				super(VerificationRoundTripTest.TestChangeVerifier, self).verify_change(verification_config, change_id, repo_type, workers_spawned, patch_id)
 			finally:
 				self._change_finished.send()
 
@@ -144,7 +144,7 @@ class VerificationRoundTripTest(BaseIntegrationTest, ModelServerTestMixin, Rabbi
 		init_commit = self._git_modify_commit_push(work_repo, modfile, contents, parent_commits=[])
 
 		with model_server.rpc_connect("changes", "create") as client:
-			commit_id = client.create_commit_and_change(self.repo_id, self.user_id, None, bare_repo.head.commit.hexsha, 'master', False)['commit_id']
+			commit_id = client.create_commit_and_change(self.repo_id, self.user_id, 'commit_message', 'sha', 'master', None, False)['commit_id']
 
 		commit_sha = self._git_modify_commit_push(work_repo, "koality.yml",
 			yaml.safe_dump({'test': {'scripts': self._test_commands(passes)}}),
@@ -198,7 +198,7 @@ class VerificationRoundTripTest(BaseIntegrationTest, ModelServerTestMixin, Rabbi
 		work_repo.git.reset('HEAD~', hard=True)
 
 		with model_server.rpc_connect("changes", "create") as client:
-			client.create_commit_and_change(self.repo_id, self.user_id, None, bare_repo.head.commit.hexsha, 'master', False, patch)
+			client.create_commit_and_change(self.repo_id, self.user_id, 'commit_message', 'sha', 'master', None, False, patch)
 
 		with Connection(RabbitSettings.kombu_connection_info) as connection:
 			Queue("verification:repos.update", EventsBroker.events_exchange, routing_key="repos", durable=False)(connection).declare()
diff --git a/ci/platform/tests/integration_tests/model_server_handler_tests.py b/ci/platform/tests/integration_tests/model_server_handler_tests.py
index e0c7816..b830b7f 100644
--- a/ci/platform/tests/integration_tests/model_server_handler_tests.py
+++ b/ci/platform/tests/integration_tests/model_server_handler_tests.py
@@ -45,17 +45,13 @@ class ModelServerHandlerTest(BaseIntegrationTest):
 			self.repo_id = sqlconn.execute(ins_repo).inserted_primary_key[0]
 
 			self.commit_sha = '0123456789abcdef'
-			self.commit_owner = "Max Power"
-			self.commit_email = "MaxPower's email"
 
 			ins_commit = commit.insert().values(
 				repo_id=self.repo_id,
 				user_id=self.user_id,
 				message='a',
 				sha=self.commit_sha,
-				timestamp=1,
-				committer_name=self.commit_owner,
-				committer_email=self.commit_email
+				timestamp=1
 			)
 
 			commit_id = sqlconn.execute(ins_commit).inserted_primary_key[0]
@@ -112,7 +108,7 @@ class ModelServerHandlerTest(BaseIntegrationTest):
 
 		for i in self.build_ids:
 			update_handler.add_subtype(i, ConsoleType.Test, "unittest")
-			update_handler.store_xunit_contents(i, ConsoleType.Test, "unittest",
+			update_handler.store_xunit_contents(i, ConsoleType.Test, "unittest", 
 				{"file": "contents1", "file2": "contents2"})
 
 	def test_store_patch(self):
diff --git a/ci/platform/tests/integration_tests/repostore_tests.py b/ci/platform/tests/integration_tests/repostore_tests.py
index 228f939..799adfd 100644
--- a/ci/platform/tests/integration_tests/repostore_tests.py
+++ b/ci/platform/tests/integration_tests/repostore_tests.py
@@ -135,7 +135,7 @@ class RepoStoreTests(BaseIntegrationTest, ModelServerTestMixin, RepoStoreTestMix
 
 		self.store.merge_changeset(self.hg_repo_id, "hgrepo", new_sha, new_sha)
 
-		assert_equals(hglib.open(self.hg_repo_path).tip()[5], "Merging in %s" % new_sha)
+		assert_equals(hglib.open(self.hg_repo_path).tip()[5], "Merging in %s" % new_sha[:12])
 
 	def test_git_merge_fail(self):
 		self.store.create_repository(self.git_repo_id, "gitrepo")
diff --git a/ci/platform/util/restricted_shell.py b/ci/platform/util/restricted_shell.py
index 69ddcdf..53aef06 100644
--- a/ci/platform/util/restricted_shell.py
+++ b/ci/platform/util/restricted_shell.py
@@ -53,7 +53,7 @@ class RestrictedSSHForwardingShell(RestrictedShell):
 		if virtual_machine is None or virtual_machine.instance.id != vm_instance_id:
 			raise VirtualMachineNotFoundError(vm_instance_id)
 
-		ssh_args = virtual_machine.ssh_args().to_arg_list()
+		ssh_args = virtual_machine.ssh_args()
 		os.execlp(ssh_args[0], *ssh_args)
 
 class RestrictedGitShell(RestrictedShell):
@@ -203,8 +203,7 @@ class RestrictedHgShell(RestrictedShell):
 		args = self.rp_new_sshargs("hg -R", requested_repo_uri, user_id)
 		os.execlp(*args)
 
-	# TODO(andrey) refactor this and cat-bundle
-	def handle_show_koality(self, requested_repo_uri, user_id, sha, file_name):
+	def handle_show_koality(self, requested_repo_uri, user_id, sha):
 		with model_server.rpc_connect("repos", "read") as modelserver_rpc_conn:
 			attributes = modelserver_rpc_conn.get_repo_attributes(requested_repo_uri)
 
@@ -214,9 +213,9 @@ class RestrictedHgShell(RestrictedShell):
 		self.verify_user_exists("git-show", user_id, attributes['repo']['id'])
 		remote_filesystem_path = os.path.join(attributes['repostore']['repositories_path'], pathgen.to_path(attributes['repo']['id'], attributes['repo']['name']))
 
-		bundle_path = os.path.join(remote_filesystem_path, ".hg", "strip-backup", sha + ".hg")
+		yml_path = os.path.join(remote_filesystem_path, ".hg", "strip-backup", sha + "-koality.yml")
 		uri = "git@%s" % attributes['repostore']['ip_address']
-		full_command = "sh -c %s" % pipes.quote("cd %s && hg -R %s cat -r tip %s" % (remote_filesystem_path, bundle_path, file_name))
+		full_command = "sh -c %s" % pipes.quote("cat %s" % yml_path)
 		os.execlp("ssh", "ssh", "-p", "2222", "-oStrictHostKeyChecking=no", uri, full_command)
 
 	def handle_cat_bundle(self, requested_repo_uri, user_id, sha):
@@ -250,7 +249,7 @@ class RestrictedHgShell(RestrictedShell):
 		if command_parts[:2] == ['hg', '-R'] and command_parts[3:5] == ['serve', '--stdio']:
 			self.handle_push(repo_path, command_parts[5])
 		elif command_parts[:2] == ['hg', 'show-koality']:
-			self.handle_show_koality(repo_path, command_parts[5], command_parts[3], command_parts[4])
+			self.handle_show_koality(repo_path, command_parts[4], command_parts[3])
 		elif command_parts[:2] == ['hg', 'cat-bundle']:
 			self.handle_cat_bundle(repo_path, command_parts[4], command_parts[3])
 		else:
diff --git a/ci/platform/util/sql.py b/ci/platform/util/sql.py
index bb3b238..fffbb57 100644
--- a/ci/platform/util/sql.py
+++ b/ci/platform/util/sql.py
@@ -5,9 +5,9 @@ from database.engine import ConnectionFactory
 
 def to_dict(row, columns, tablename=None):
 	if not tablename:
-		return { col.name: getattr(row, col.name) for col in columns } if row else {}
+		return dict([(col.name, getattr(row, col.name)) for col in columns]) if row else {}
 	else:
-		return { col.name: getattr(row, '%s_%s' % (tablename, col.name)) for col in columns } if row else {}
+		return dict([(col.name, getattr(row, '_'.join([tablename, col.name]))) for col in columns]) if row else {}
 
 
 def load_temp_strings(strings):
diff --git a/ci/platform/util/test/fake_build_verifier.py b/ci/platform/util/test/fake_build_verifier.py
index 964add7..83ad155 100644
--- a/ci/platform/util/test/fake_build_verifier.py
+++ b/ci/platform/util/test/fake_build_verifier.py
@@ -1,5 +1,9 @@
 import os
 import pipes
+import shlex
+
+from database import schema
+from database.engine import ConnectionFactory
 
 from verification.build_core import VirtualMachineBuildCore
 from virtual_machine.virtual_machine import VirtualMachine
@@ -14,12 +18,24 @@ class FakeBuildCore(VirtualMachineBuildCore):
 	def setup(self):
 		pass
 
+	def setup_build(self, repo_uri, repo_type, ref, private_key, patch_id=None, console_appender=None):
+		self.virtual_machine.ssh_call('git init source; cd source; git fetch %s %s; git checkout FETCH_HEAD' % (repo_uri, ref))
+		if patch_id:
+			patch = schema.patch
+			with ConnectionFactory.get_sql_connection() as sqlconn:
+				query = patch.select().where(patch.c.id == patch_id)
+				row = sqlconn.execute(query).first()
+				assert row
+				assert row[patch.c.id] == patch_id
+
+				assert self.virtual_machine.ssh_call('cd source && echo %s | patch -p1' % pipes.quote(str(row[patch.c.contents]))).returncode == 0
+
 	def teardown(self):
 		self.virtual_machine.ssh_call('rm -rf source')
 		self.virtual_machine.delete()
 
 	def cache_repository(self, repo_uri):
-		return self.virtual_machine.call('true')
+		return self.virtual_machine.call(["true"])
 
 
 class FakeVirtualMachine(VirtualMachine):
@@ -31,20 +47,14 @@ class FakeVirtualMachine(VirtualMachine):
 		super(FakeVirtualMachine, self).__init__(vm_id, FakeVirtualMachine.Instance(vm_id), 'fakeusername')
 		self.vm_working_dir = os.path.abspath(os.path.join('/', 'tmp', 'fakevm_%s' % self.instance.id))
 
-	def configure_ssh(self, private_key, output_handler=None):
-		return self.call('true')
-
-	def remote_checkout(self, repo_name, repo_url, repo_type, ref, output_handler=None):
-		return self.ssh_call('git init %s; cd %s; git fetch %s %s; git checkout FETCH_HEAD' % (repo_name, repo_name, repo_url, ref))
-
-	def provision(self, repo_name, environment, language_config, setup_config, output_handler=None):
-		return self.call('true')
+	def provision(self, repo_name, private_key, output_handler=None):
+		return self.call(["true"])
 
 	def ssh_call(self, command, output_handler=None, timeout=None):
-		return self.call('bash -c %s' % pipes.quote('mkdir -p %s; cd %s; %s' % (self.vm_working_dir, self.vm_working_dir, str(command))), output_handler, timeout=timeout)
+		return self.call(shlex.split('bash -c %s' % pipes.quote('mkdir -p %s; cd %s; %s' % (self.vm_working_dir, self.vm_working_dir, str(command)))), output_handler, timeout=timeout)
 
 	def export(self, export_prefix, files, output_handler=None):
-		return self.call('true')
+		return self.call(["true"])
 
 	def delete(self):
-		return self.call('rm -rf %s' %self.vm_working_dir)
+		return self.call(['rm', '-rf', self.vm_working_dir])
diff --git a/ci/platform/util/test/fake_data_generator.py b/ci/platform/util/test/fake_data_generator.py
index ed37baa..00db700 100644
--- a/ci/platform/util/test/fake_data_generator.py
+++ b/ci/platform/util/test/fake_data_generator.py
@@ -104,8 +104,9 @@ class SchemaDataGenerator(object):
 				first_name = random.choice(['Jon', 'Jordan', 'Brian', 'Ryan', 'Andrey'])
 				last_name = random.choice(['Chu', 'Potter', 'Bland', 'Scott', 'Kostov'])
 
-				user_email = "%s%d@address.com" % (first_name[0] + last_name, user)
-				ins_user = schema.user.insert().values(first_name=first_name, last_name=last_name, email=user_email,
+
+				ins_user = schema.user.insert().values(first_name=first_name, last_name=last_name,
+					email="%s%d@address.com" % (first_name[0] + last_name, user),
 					password_hash=binascii.b2a_base64(hashlib.sha512(SALT + USER_PASSWORD.encode('utf8')).digest())[0:-1], salt=SALT, created=int(time.time()))
 				user_id = conn.execute(ins_user).inserted_primary_key[0]
 
@@ -118,11 +119,8 @@ class SchemaDataGenerator(object):
 					sha = ''.join(random.choice('0123456789abcdef') for x in range(40))
 					commit_message = 'Jon %s while %s' % (random.choice(['ate', 'fell', 'exploded', 'watched tv']),
 							random.choice(['listening to Selena Gomez\n\nBut only on MTV', 'chewing gum\n\nDouble the pleasure, double the fun', 'praying to Raptor Jesus']))
-					commit_owner = "%s" % first_name + ' ' + last_name
-					commit_email = user_email
 					ins_commit = schema.commit.insert().values(repo_id=repo_id, user_id=user_id,
-						message=commit_message, timestamp=random.randint(1, int(time.time())), sha=sha,
-						committer_name=commit_owner, committer_email=commit_email)
+						message=commit_message, timestamp=random.randint(1, int(time.time())), sha=sha)
 					commit_id = conn.execute(ins_commit).inserted_primary_key[0]
 					ins_change = schema.change.insert().values(commit_id=commit_id, repo_id=repo_id, merge_target="target-%d" % commit,
 						number=repos[repo_id], verification_status=self.get_random_commit_status(), merge_status=self.get_random_merge_status(),
diff --git a/ci/platform/verification/build_core.py b/ci/platform/verification/build_core.py
index 8ba2dc8..9494ad5 100644
--- a/ci/platform/verification/build_core.py
+++ b/ci/platform/verification/build_core.py
@@ -5,7 +5,7 @@ import model_server
 
 from model_server.build_consoles import ConsoleType
 from util.log import Logged
-from virtual_machine.remote_command import RemoteTestCommand, RemoteExportCommand, InvalidConfigurationException
+from virtual_machine.remote_command import RemoteCheckoutCommand, RemotePatchCommand, RemoteExportCommand, RemoteProvisionCommand, RemoteTestCommand, InvalidConfigurationException
 
 
 @Logged()
@@ -23,11 +23,17 @@ class VirtualMachineBuildCore(object):
 	def rebuild(self):
 		raise NotImplementedError()
 
+	def setup_build(self, repo_uri, repo_type, ref, private_key, patch_id=None, console_appender=None):
+		repo_name = self.uri_translator.extract_repo_name(repo_uri)
+		self.setup_virtual_machine(repo_name, private_key, console_appender)
+
 	def _get_output_handler(self, console_appender, type, subtype=""):
 		return console_appender(type, subtype) if console_appender else None
 
-	def run_setup_step(self, setup_commands, console_appender=None):
+	def setup_virtual_machine(self, repo_name, private_key, console_appender, setup_commands=[]):
 		"""Provisions the contained virtual machine for analysis and test running"""
+		provision_command = RemoteProvisionCommand(repo_name, private_key)
+		setup_commands = setup_commands + [provision_command]
 		for setup_command in setup_commands:
 			self.run_setup_command(setup_command, console_appender)
 
@@ -42,7 +48,7 @@ class VirtualMachineBuildCore(object):
 		for compile_command in compile_commands:
 			self.run_compile_command(compile_command, console_appender)
 
-	def run_compile_command(self, compile_command, console_appender):
+	def run_compile_command(self, compile_command, console_appender=None):
 		results = compile_command.run(self.virtual_machine,
 			self._get_output_handler(console_appender, ConsoleType.Compile, compile_command.name))
 		if results.returncode:
@@ -87,11 +93,6 @@ class VirtualMachineBuildCore(object):
 		except yaml.YAMLError as e:
 			factory_failures(e)
 
-		if test_sections is None:
-			if output_handler:
-				output_handler.append({1: 'No test sections generated'})
-			return []
-
 		if not isinstance(test_sections, list):
 			test_sections = [test_sections]
 
@@ -154,6 +155,17 @@ class CloudBuildCore(VirtualMachineBuildCore):
 	def rebuild(self):
 		self.virtual_machine.rebuild()
 
+	def setup_build(self, repo_uri, repo_type, ref, private_key, patch_id=None, console_appender=None):
+		self.setup_virtual_machine(repo_uri, repo_type, ref, private_key, patch_id, console_appender)
+
+	def setup_virtual_machine(self, repo_uri, repo_type, ref, private_key, patch_id, console_appender):
+		checkout_url = self.uri_translator.translate(repo_uri)
+		repo_name = self.uri_translator.extract_repo_name(repo_uri)
+		commands = [RemoteCheckoutCommand(repo_name, checkout_url, repo_type, ref)]
+		if patch_id:
+			commands.append(RemotePatchCommand(repo_name, patch_id))
+		super(CloudBuildCore, self).setup_virtual_machine(repo_name, private_key, console_appender, commands)
+
 
 class VerificationException(Exception):
 	"""Default exception to be thrown during verification failure.
diff --git a/ci/platform/verification/build_verifier.py b/ci/platform/verification/build_verifier.py
index 5833a87..515bef5 100644
--- a/ci/platform/verification/build_verifier.py
+++ b/ci/platform/verification/build_verifier.py
@@ -1,10 +1,13 @@
+import os
 import yaml
 
 import model_server
 
 from build_core import VerificationException
 from pubkey_registrar import PubkeyRegistrar
+from settings.store import StoreSettings
 from shared.constants import BuildStatus, VerificationUser
+from util import pathgen
 from util.log import Logged
 from verification_results_handler import VerificationResultsHandler
 
@@ -88,7 +91,18 @@ class BuildVerifier(object):
 	def launch_build(self, commit_id, repo_type, verification_config):
 		self.build_core.virtual_machine.store_vm_metadata(commit_id=commit_id)
 
-		self.build_core.run_setup_step(verification_config.setup_commands)
+		repo_uri = self._get_repo_uri(commit_id)
+
+		if repo_type == "git":
+			ref = pathgen.hidden_ref(commit_id)
+		elif repo_type == "hg":
+			ref = self._get_commit(commit_id)['sha']
+		else:
+			raise NoSuchRepoTypeError("Unknown repository type %s." % repo_type)
+
+		private_key = StoreSettings.ssh_private_key
+
+		self.build_core.setup_build(repo_uri, repo_type, ref, private_key)
 		self.build_core.run_compile_step(self._dedupe_step_names(verification_config.compile_commands))
 
 	@ReturnException
@@ -97,10 +111,19 @@ class BuildVerifier(object):
 		commit_id = build['commit_id']
 		self.logger.info("Worker %s processing verification request: (build id: %s, commit id: %s)" % (self.worker_id, build_id, commit_id))
 		self._start_build(build_id)
+		repo_uri = self._get_repo_uri(commit_id)
+
+		if repo_type == "git":
+			ref = pathgen.hidden_ref(commit_id)
+		elif repo_type == "hg":
+			ref = self._get_commit(commit_id)['sha']
+		else:
+			raise NoSuchRepoTypeError("Unknown repository type %s." % repo_type)
 
+		private_key = StoreSettings.ssh_private_key
 		with model_server.rpc_connect("build_consoles", "update") as build_consoles_update_rpc:
 			console_appender = self._make_console_appender(build_consoles_update_rpc, build_id)
-			self.build_core.run_setup_step(verification_config.setup_commands, console_appender)
+			self.build_core.setup_build(repo_uri, repo_type, ref, private_key, patch_id, console_appender)
 			self.build_core.run_compile_step(self._dedupe_step_names(verification_config.compile_commands), console_appender)
 
 	@ReturnException
@@ -108,7 +131,7 @@ class BuildVerifier(object):
 		test_queue.begin_populating_tasks()
 		try:
 			test_commands = verification_config.test_commands
-			for factory_command in self._dedupe_step_names(verification_config.test_factory_commands):
+			for factory_command in verification_config.test_factory_commands:
 				with model_server.rpc_connect("build_consoles", "update") as build_consoles_update_rpc:
 					console_appender = self._make_console_appender(build_consoles_update_rpc, build_id)
 					generated_test_commands = self.build_core.run_factory_command(factory_command, console_appender)
diff --git a/ci/platform/verification/change_verifier.py b/ci/platform/verification/change_verifier.py
index 3551802..2c4635a 100644
--- a/ci/platform/verification/change_verifier.py
+++ b/ci/platform/verification/change_verifier.py
@@ -1,9 +1,8 @@
-import collections
-import os
 import subprocess
 import sys
 
 import yaml
+import os
 
 from eventlet import event, spawn, spawn_n, spawn_after, queue
 from kombu.messaging import Producer
@@ -11,11 +10,8 @@ from kombu.messaging import Producer
 import model_server
 
 from build_core import VerificationException
-from pubkey_registrar import PubkeyRegistrar
-from shared.constants import VerificationUser
 from shared.handler import EventSubscriber, ResourceBinding
 from settings.deployment import DeploymentSettings
-from settings.store import StoreSettings
 from settings.verification_server import VerificationServerSettings
 from util import pathgen
 from util.log import Logged
@@ -51,21 +47,18 @@ class ChangeVerifier(EventSubscriber):
 		try:
 			change_id = contents['change_id']
 			commit_id = contents['commit']['id']
-			head_sha = contents['commit']['sha']
-			base_sha = contents['commit'].get('base_sha')
+			sha = contents['commit']['sha']
 			repo_type = contents['repo_type']
-			merge_target = contents['merge_target']
 			patch_id = contents['patch_id']
-			verify_only = contents['verify_only']
 
-			if contents['skip'] or (not DeploymentSettings.active):
+			if not DeploymentSettings.active:
 				self.skip_change(change_id)
 				return
 
-			verification_config = self._get_verification_config(commit_id, head_sha, base_sha, merge_target, repo_type, patch_id)
+			verification_config = self._get_verification_config(commit_id, sha, repo_type)
 
 			workers_spawned = event.Event()
-			spawn_n(self.verify_change, verification_config, change_id, repo_type, workers_spawned, verify_only, patch_id)
+			spawn_n(self.verify_change, verification_config, change_id, repo_type, workers_spawned, patch_id)
 			workers_spawned.wait()
 		except:
 			self.logger.critical("Unexpected failure while verifying change %d, commit %d. Failing change." % (change_id, commit_id), exc_info=True)
@@ -102,18 +95,14 @@ class ChangeVerifier(EventSubscriber):
 					change_attributes = client.get_change_attributes(change_id)
 
 				commit_id = change_attributes['commit_id']
-				merge_target = change_attributes['merge_target']
 
 				with model_server.rpc_connect("repos", "read") as client:
 					repo_type = client.get_repo_type(change_attributes['repo_id'])
-					commit_attributes = client.get_commit_attributes(commit_id)
-
-				head_sha = commit_attributes['sha']
-				base_sha = commit_attributes['base_sha']
+					sha = client.get_commit_attributes(commit_id)['sha']
 
 				user_id = contents['user_id']
 
-				verification_config = self._get_verification_config(commit_id, head_sha, base_sha, merge_target, repo_type, None)
+				verification_config = self._get_verification_config(commit_id, sha, repo_type)
 
 				debug_instance = spawn(verifier.launch_build, commit_id, repo_type, verification_config)
 
@@ -136,7 +125,7 @@ class ChangeVerifier(EventSubscriber):
 	def skip_change(self, change_id):
 		self.results_handler.skip_change(change_id)
 
-	def verify_change(self, verification_config, change_id, repo_type, workers_spawned, verify_only, patch_id=None):
+	def verify_change(self, verification_config, change_id, repo_type, workers_spawned, patch_id=None):
 		task_queue = TaskQueue()
 
 		num_workers = self._get_num_workers(verification_config)
@@ -166,9 +155,9 @@ class ChangeVerifier(EventSubscriber):
 			with model_server.rpc_connect("changes", "update") as model_server_rpc:
 				model_server_rpc.mark_change_started(change_id)
 
-		def pass_change(verify_only):
+		def pass_change():
 			change_done.send(True)
-			self.results_handler.pass_change(change_id, verify_only)
+			self.results_handler.pass_change(change_id)
 
 		def fail_change():
 			change_done.send(False)
@@ -236,7 +225,7 @@ class ChangeVerifier(EventSubscriber):
 				fail_change()
 				change_failed = True
 		if not change_failed:
-			pass_change(verify_only)
+			pass_change()
 
 	def _get_num_workers(self, verification_config):
 		if verification_config.test_factory_commands:
@@ -256,16 +245,13 @@ class ChangeVerifier(EventSubscriber):
 		with model_server.rpc_connect("builds", "create") as model_server_rpc:
 			return model_server_rpc.create_build(change_id)
 
-	def _get_verification_config(self, commit_id, head_sha, base_sha, merge_target, repo_type, patch_id):
+	def _get_verification_config(self, commit_id, sha, repo_type):
 		with model_server.rpc_connect("repos", "read") as model_server_rpc:
 			repo_uri = model_server_rpc.get_repo_uri(commit_id)
 			attributes = model_server_rpc.get_repo_attributes(repo_uri)
 
 		repo_name = attributes['repo']['name']
 
-		PubkeyRegistrar().register_pubkey(VerificationUser.id, "ChangeVerifier")
-		PubkeyRegistrar().register_pubkey(VerificationUser.id, "Koality Keypair", StoreSettings.ssh_public_key)
-
 		config_dict = {}
 
 		if repo_type == 'git':
@@ -277,46 +263,40 @@ class ChangeVerifier(EventSubscriber):
 				repo_path = checkout_url[checkout_url.find(":") + 1:]
 				show_command = lambda file_name: ["ssh", "-q", "-oStrictHostKeyChecking=no", host_url, "git-show", repo_path, "%s:%s" % (ref, file_name)]
 			else:
-				checkout_url = repo_uri
 				show_command = lambda file_name: ["bash", "-c", "cd %s && git show %s:%s" % (repo_uri, ref, file_name)]
+
+			for file_name in ['koality.yml', '.koality.yml']:
+				try:
+					config_dict = yaml.safe_load(subprocess.check_output(show_command(file_name)))
+				except:
+					pass
+				else:
+					break
 		elif repo_type == 'hg':
-			ref = head_sha
 			if self.uri_translator:
 				checkout_url = self.uri_translator.translate(repo_uri)
 				host_url, _, repo_uri = checkout_url.split('://')[1].partition('/')
-				show_command = lambda file_name: ["ssh", "-q", "-oStrictHostKeyChecking=no", host_url, "hg", "show-koality", repo_uri, ref, file_name]
+				show_command = ["ssh", "-q", "-oStrictHostKeyChecking=no", host_url, "hg", "show-koality", repo_uri, sha]
 			else:
 				# TODO(andrey) Test this case!
-				checkout_url = repo_uri
-				show_command = lambda file_name: ["bash", "-c", "cd %s && hg -R %s cat -r tip %s" % ((repo_uri, os.path.join(repo_uri, ".hg", "strip-backup", head_sha + ".hg")), file_name)]
+				show_command = ["bash", "-c", "cat %s" % os.path.join(repo_uri, ".hg", "strip-backup", sha + "-koality.yml")]
+			try:
+				config_dict = yaml.safe_load(subprocess.check_output(show_command))
+			except:
+				pass
+
 		else:
 			self.logger.critical()
 			assert False
 
-		for file_name in ['koality.yml', '.koality.yml']:
-				try:
-					config_dict =  yaml.safe_load(subprocess.check_output(show_command(file_name)))
-				except:
-					pass
-				else:
-					break
-
 		if not isinstance(config_dict, dict):
 			config_dict = {}
 
-		environment = collections.OrderedDict()
-		environment['KOALITY']  = 'true'
-		environment['KOALITY_HEAD_SHA'] = head_sha
-		if base_sha:
-			environment['KOALITY_BASE_SHA'] = base_sha
-		environment['KOALITY_BRANCH'] = merge_target
-		environment['KOALITY_REPOSITORY'] = repo_name
-
 		try:
-			return VerificationConfig(repo_type, repo_name, checkout_url, ref, environment, config_dict, private_key=StoreSettings.ssh_private_key, patch_id=patch_id)
+			return VerificationConfig(repo_name, config_dict.get("compile"), config_dict.get("test"), config_dict.get("export"))
 		except:
 			self.logger.critical("Unexpected exception while getting verification configuration", exc_info=True)
-			return VerificationConfig(repo_type, repo_name, checkout_url, ref, environment, {})
+			return VerificationConfig(repo_name, {}, {})
 
 	def _is_result_failed(self, result):
 		return isinstance(result, Exception)
diff --git a/ci/platform/verification/verification_config.py b/ci/platform/verification/verification_config.py
index 32fd3ff..c38bdc5 100644
--- a/ci/platform/verification/verification_config.py
+++ b/ci/platform/verification/verification_config.py
@@ -1,45 +1,32 @@
-from virtual_machine.remote_command import RemoteProvisionCommand, RemoteCompileCommand, RemoteTestCommand, RemoteTestFactoryCommand, RemoteErrorCommand, RemoteCheckoutCommand, RemotePatchCommand, RemoteSshConfigCommand
+import os
+
+from virtual_machine.remote_command import RemoteCompileCommand, RemoteTestCommand, RemoteTestFactoryCommand, RemoteErrorCommand
 
 
 class VerificationConfig(object):
-	def __init__(self, repo_type, repo_name, repo_url, ref, environment, configuration, private_key=None, patch_id=None):
+	def __init__(self, repo_name, compile_section, test_section, export_section):
 		try:
 			self.repo_name = repo_name
-
-			config = configuration or {}
-			language_section = config.get('languages')
-			setup_section = config.get('setup')
-			compile_section = config.get('compile')
-			test_section = config.get('test')
-			export_section = config.get('export')
-
-			setup_commands = []
-			if private_key is not None:
-				setup_commands.append(RemoteSshConfigCommand(private_key))
-			setup_commands.append(RemoteCheckoutCommand(repo_name, repo_url, repo_type, ref))
-			if patch_id is not None:
-				setup_commands.append(RemotePatchCommand(repo_name, patch_id))
-			setup_commands.append(RemoteProvisionCommand(repo_name, environment, language_section, setup_section))
-
-			self.setup_commands = setup_commands
-
 			self.machines = test_section.get('machines')
-
 			compile_commands = compile_section.get('scripts') if compile_section else None
 			test_commands = test_section.get('scripts') if test_section else None
 			test_factory_commands = test_section.get('factories') if test_section else None
-
 			self.compile_commands = self._convert(RemoteCompileCommand, compile_commands)
 			self.test_commands = self._convert(RemoteTestCommand, test_commands)
 			self.test_factory_commands = self._convert(RemoteTestFactoryCommand, test_factory_commands)
-
-			self.export_paths = self._to_export_paths(export_section)
+			if export_section is None:
+				self.export_paths = []
+			elif isinstance(export_section, str):
+				self.export_paths = [export_section]
+			elif isinstance(export_section, list):
+				self.export_paths = export_section
+			else:
+				assert False, 'Export section must be a string or list of strings'
+			assert all(map(lambda path: isinstance(path, str), self.export_paths)), 'Export section must be a string or list of strings'
 		except:
 			self.machines = 1
-			# TODO (bbland): record all failures and display this nicely to the users
-			self.setup_commands = [RemoteErrorCommand("parse error", "Could not parse your koality.yml file.\nPlease verify that it is valid yaml and matches the expected format.")]
 			self.compile_commands = []
-			self.test_commands = []
+			self.test_commands = [RemoteErrorCommand("parse error", "Could not parse your koality.yml file.\nPlease verify that it is valid yaml and matches the expected format.")]
 			self.test_factory_commands = []
 			self.export_paths = []
 
@@ -60,18 +47,6 @@ class VerificationConfig(object):
 	def _convert_dict(self, command_class, commands):
 		return self._convert_list(command_class, map(lambda entry: dict((entry,)), commands.iteritems()))
 
-	def _to_export_paths(self, export_section):
-		if export_section is None:
-			export_paths = []
-		elif isinstance(export_section, str):
-			export_paths = [export_section]
-		elif isinstance(export_section, list):
-			export_paths = export_section
-		else:
-			assert False, 'Export section must be a string or list of strings'
-		assert all(map(lambda path: isinstance(path, str), export_paths)), 'Export section must be a string or list of strings'
-		return export_paths
-
 
 class InvalidConfigurationException(Exception):
 	pass
diff --git a/ci/platform/verification/verification_results_handler.py b/ci/platform/verification/verification_results_handler.py
index 9fa2c15..2f43f57 100644
--- a/ci/platform/verification/verification_results_handler.py
+++ b/ci/platform/verification/verification_results_handler.py
@@ -13,10 +13,9 @@ class VerificationResultsHandler(object):
 	def __init__(self):
 		self.remote_repo_manager = DistributedLoadBalancingRemoteRepositoryManager(ConnectionFactory.get_redis_connection('repostore'))
 
-	def pass_change(self, change_id, verify_only):
-		if verify_only:
-			self._set_change_status_if_not_finished(change_id, BuildStatus.PASSED)
-		elif self._send_merge_request(change_id, BuildStatus.PASSED):
+	def pass_change(self, change_id):
+		merge_success = self._send_merge_request(change_id, BuildStatus.PASSED)
+		if merge_success:
 			self._set_change_status_if_not_finished(change_id, BuildStatus.PASSED, MergeStatus.PASSED)
 
 	def skip_change(self, change_id):
@@ -42,6 +41,9 @@ class VerificationResultsHandler(object):
 		commit_id = change_attributes['commit_id']
 		merge_target = change_attributes['merge_target']
 
+		if merge_target == 'verify only':
+			return True
+
 		with model_server.rpc_connect("repos", "read") as client:
 			repo_uri = client.get_repo_uri(commit_id)
 			commit_attributes = client.get_commit_attributes(commit_id)
diff --git a/ci/platform/virtual_machine/ec2.py b/ci/platform/virtual_machine/ec2.py
index 52e9ab3..d116240 100644
--- a/ci/platform/virtual_machine/ec2.py
+++ b/ci/platform/virtual_machine/ec2.py
@@ -10,8 +10,6 @@ import boto.ec2
 import eventlet
 
 import model_server
-
-from pysh.shell_tools import ShellCommand, ShellChain, ShellAppend, ShellRedirect, ShellOr
 from settings.aws import AwsSettings
 from util.log import Logged
 from verification.pubkey_registrar import PubkeyRegistrar
@@ -69,15 +67,7 @@ class Ec2Vm(VirtualMachine):
 	@classmethod
 	def construct(cls, vm_id, name=None, ami_image_id=None, instance_type=None, vm_username=VM_USERNAME):
 		if not name:
-			master_name = None
-			try:
-				master_instance_id = cls._call(['ec2metadata', '--instance-id']).output.strip()
-				master_name = cls.CloudClient().get_all_instances(filters={'instance-id': master_instance_id})[0].instances[0].tags['Name']
-			except:
-				pass
-			if not master_name:
-				master_name = socket.gethostname()
-			name = "koality-worker:%s (%s)" % (vm_id, master_name)
+			name = "koality:%s:%s" % (socket.gethostname(), vm_id)
 		if not ami_image_id:
 			ami_image_id = cls.get_newest_image().id
 		if not instance_type:
@@ -104,19 +94,11 @@ class Ec2Vm(VirtualMachine):
 		when it finishes first boot.
 		This will fail if we use an image which doesn't utilitize EC2 user_data
 		'''
-		return '#!/bin/sh\n%s' % ShellChain(
-				ShellCommand('useradd --create-home %s' % vm_username),
-				ShellCommand('mkdir ~%s/.ssh' % vm_username),
-				ShellAppend('echo %s' % pipes.quote(PubkeyRegistrar().get_ssh_pubkey()), '~%s/.ssh/authorized_keys' % vm_username),
-				ShellCommand('chown -R %s:%s ~%s/.ssh' % (vm_username, vm_username, vm_username)),
-				ShellOr(
-					ShellCommand("grep '#includedir /etc/sudoers.d' /etc/sudoers"),
-					ShellAppend('echo #includedir /etc/sudoers.d', '/etc/sudoers')
-				),
-				ShellCommand('mkdir /etc/sudoers.d'),
-				ShellRedirect("echo '%s ALL=(ALL) NOPASSWD: ALL'" % vm_username, '/etc/sudoers.d/koality-%s' % vm_username),
-				ShellCommand('chmod 0440 /etc/sudoers.d/koality-%s' % vm_username)
-			)
+		return '\n'.join(("#!/bin/sh",
+			"useradd --create-home %s" % vm_username,
+			"mkdir ~%s/.ssh" % vm_username,
+			"echo '%s' >> ~%s/.ssh/authorized_keys" % (PubkeyRegistrar().get_ssh_pubkey(), vm_username),
+			"chown -R %s:%s ~%s/.ssh" % (vm_username, vm_username, vm_username)))
 
 	@classmethod
 	def _validate_security_group(cls, security_group):
@@ -235,6 +217,10 @@ class Ec2Vm(VirtualMachine):
 		with model_server.rpc_connect("debug_instances", "create") as debug_create_rpc:
 			debug_create_rpc.create_vm_in_db("Ec2Vm", self.instance.id, self.vm_id, self.vm_username)
 
+	def provision(self, repo_name, private_key, output_handler=None):
+		return self.ssh_call("PYTHONUNBUFFERED=true koality-provision %s %s" % (pipes.quote(repo_name), pipes.quote(private_key)),
+			timeout=3600, output_handler=output_handler)
+
 	def export(self, repo_name, export_prefix, file_paths, output_handler=None):
 		export_options = {
 			'provider': 's3',
@@ -250,13 +236,10 @@ class Ec2Vm(VirtualMachine):
 		)
 
 	def ssh_args(self):
-		options = {
-			'LogLevel': 'error',
-			'StrictHostKeyChecking': 'no',
-			'UserKnownHostsFile': '/dev/null',
-			'ServerAliveInterval': '20'
-		}
-		return self.SshArgs(self.vm_username, self.instance.private_ip_address, options=options)
+		return ["ssh",
+			"-oLogLevel=error", "-oStrictHostKeyChecking=no",
+			"-oUserKnownHostsFile=/dev/null", "-oServerAliveInterval=20",
+			"%s@%s" % (self.vm_username, self.instance.private_ip_address)]
 
 	def reboot(self, force=False):
 		self.instance.reboot()
@@ -305,7 +288,6 @@ class Ec2Vm(VirtualMachine):
 		else:
 			self._safe_terminate(self.instance)
 		self.remove_vm_info()
-		super(Ec2Vm, self).delete()
 
 	def _safe_terminate(self, instance):
 		try:
diff --git a/ci/platform/virtual_machine/hpcloud.py b/ci/platform/virtual_machine/hpcloud.py
index 2d1cb45..b3fc324 100644
--- a/ci/platform/virtual_machine/hpcloud.py
+++ b/ci/platform/virtual_machine/hpcloud.py
@@ -45,13 +45,10 @@ class HpCloudVm(openstack.OpenstackVm):
 	def ssh_args(self):
 		# For some reason, hpcloud returns [<private ip>, <public ip>] as the private ips, and the private ips begin with 10.*
 		private_ip = filter(lambda ip_address: ip_address.startswith('10.'), self.instance.private_ips)[0]
-		options = {
-			'LogLevel': 'error',
-			'StrictHostKeyChecking': 'no',
-			'UserKnownHostsFile': '/dev/null',
-			'ServerAliveInterval': '20'
-		}
-		return self.SshArgs(self.vm_username, private_ip, options=options)
+		return ["ssh",
+			"-oLogLevel=error", "-oStrictHostKeyChecking=no",
+			"-oUserKnownHostsFile=/dev/null", "-oServerAliveInterval=20",
+			"%s@%s" % (self.vm_username, private_ip)]
 
 	@classmethod
 	def _get_instance_size(cls, instance_type, matching_attribute='name'):
diff --git a/ci/platform/virtual_machine/openstack.py b/ci/platform/virtual_machine/openstack.py
index 41734fd..757c8dc 100644
--- a/ci/platform/virtual_machine/openstack.py
+++ b/ci/platform/virtual_machine/openstack.py
@@ -189,13 +189,10 @@ class OpenstackVm(VirtualMachine):
 			timeout=3600, output_handler=output_handler)
 
 	def ssh_args(self):
-		options = {
-			'LogLevel': 'error',
-			'StrictHostKeyChecking': 'no',
-			'UserKnownHostsFile': '/dev/null',
-			'ServerAliveInterval': '20'
-		}
-		return self.SshArgs(self.vm_username, self.instance.private_ips[-1], options=options)
+		return ["ssh",
+			"-oLogLevel=error", "-oStrictHostKeyChecking=no",
+			"-oUserKnownHostsFile=/dev/null", "-oServerAliveInterval=20",
+			"%s@%s" % (self.vm_username, self.instance.private_ips[-1])]
 
 	def reboot(self, force=False):
 		self.instance.reboot()
@@ -253,7 +250,6 @@ class OpenstackVm(VirtualMachine):
 		else:
 			self._safe_terminate(self.instance)
 		self.remove_vm_info()
-		super(OpenstackVm, self).delete()
 
 	@classmethod
 	def _safe_terminate(cls, instance):
diff --git a/ci/platform/virtual_machine/remote_command.py b/ci/platform/virtual_machine/remote_command.py
index 14b4b06..b88834a 100644
--- a/ci/platform/virtual_machine/remote_command.py
+++ b/ci/platform/virtual_machine/remote_command.py
@@ -3,7 +3,6 @@ import os
 import pipes
 import model_server
 
-from pysh.shell_tools import ShellCommand, ShellSilent, ShellAnd, ShellOr, ShellChain, ShellTest, ShellIf, ShellAdvertised, ShellLogin, ShellBackground, ShellSubshell
 from streaming_executor import CommandResults
 
 
@@ -37,7 +36,7 @@ class RemoteShellCommand(RemoteCommand):
 		self.name, self.path, self.commands, self.timeout, self.xunit = self._parse_step(step_info)
 
 	def _run(self, virtual_machine, output_handler=None):
-		return virtual_machine.ssh_call(ShellLogin(self._to_script()), output_handler)
+		return virtual_machine.ssh_call('bash --login -c %s' % pipes.quote(self._to_script()), output_handler)
 
 	def _parse_step(self, step):
 		path = None
@@ -109,54 +108,38 @@ class RemoteShellCommand(RemoteCommand):
 			assert False, "Invalid type (%s) for value %s" % (type(value).__name__, value)
 
 	def _to_script(self):
-		def advertise(command):
-			return ShellAdvertised(command) if self.advertise_commands else ShellCommand(command)
-
-		given_command = ShellCommand('eval %s' % pipes.quote(str(ShellAnd(*map(advertise, self.commands)))))
-
+		full_command = "eval %s" % pipes.quote("&&\n".join(map(self._advertised_command, self.commands)))
+		script = "%s\n" % self._advertised_command("cd %s" % (os.path.join(self.repo_name, self.path) if self.path else self.repo_name))
 		# If timeout fails to cleanly interrupt the script in 3 seconds, we send a SIGKILL
 		timeout_message = "echo %s timed out after %s seconds" % (pipes.quote(self.name), self.timeout)
-
-		timeout_command = ShellChain(
-			ShellCommand('sleep %s' % self.timeout),
-			ShellSilent('kill -INT $$'),
-			ShellCommand('sleep 1'),
-			ShellIf(
-				ShellSilent('kill -0 $$'),
-				ShellChain(
-					ShellCommand('sleep 2'),
-					ShellSilent('kill -KILL $$'),
-					ShellCommand('echo'),
-					timeout_message,
-					ShellCommand('kill -9 0')
-				),
-				ShellChain(
-					ShellCommand('echo'),
-					timeout_message,
-					ShellCommand('kill -9 0')
-				)
-			)
-		)
-
-		commands_with_timeout = ShellChain(
-			ShellBackground(ShellSilent(ShellSubshell(timeout_command))),
-			ShellCommand('watchdogpid=$!'),
-			given_command,
-			ShellCommand('_r=$?'),
-			ShellCommand('exec 2>/dev/null'),  # goodbye stderr stream
-			ShellSilent('kill -KILL $watchdogpid'),  # kill the timeout process
-			ShellSilent('pkill -KILL -P $watchdogpid'),  # kill all children of the timeout process
-			ShellOr(
-				ShellTest('$_r -eq 0'),
-				ShellCommand('echo %s failed with return code $_r' % pipes.quote(self.name))
-			),
-			ShellCommand('exit $_r')
-		)
-
-		return ShellAnd(
-			advertise('cd %s' % (os.path.join(self.repo_name, self.path) if self.path else self.repo_name)),
-			commands_with_timeout
-		)
+		timeout_command = "\n".join((
+			"sleep %s" % self.timeout,
+			"kill -INT $$ > /dev/null 2>&1",
+			"sleep 1",  # let the process die
+			"if kill -0 $$ > /dev/null 2>&1",  # check if the process is still running
+			"then sleep 2; kill -KILL $$ > /dev/null 2>&1; echo; %s; kill -9 0" % timeout_message,  # kill forcefully
+			"else echo; %s; kill -9 0" % timeout_message,
+			"fi"
+		))
+		commands_with_timeout = "\n".join((
+			"(%s) > /dev/null 2>&1 &" % pipes.quote(timeout_command),
+			"watchdogpid=$!",
+			full_command,
+			"_r=$?",
+			"exec 2>/dev/null",  # goodbye stderr stream
+			"kill -KILL $watchdogpid > /dev/null 2>&1",  # kill our timeout process
+			"pkill -KILL -P $watchdogpid >/dev/null 2>&1",  # kill all children of the timeout process
+			"if [ $_r -ne 0 ]; then echo %s failed with return code $_r; fi" % pipes.quote(self.name),
+			"exit $_r"
+		))
+		script += "(%s)\n" % commands_with_timeout
+		return script
+
+	def _advertised_command(self, command):
+		if self.advertise_commands:
+			advertised_command = '$ ' + '\n> '.join(command.split('\n'))
+			return "echo -e %s &&\n%s" % (pipes.quote(advertised_command), command)
+		return command
 
 
 class RemoteCompileCommand(RemoteShellCommand):
@@ -202,9 +185,7 @@ for xunit_path in %s:
 contents = {}
 for file in files:
 	with open(file) as f:
-		file_contents = f.read().strip()
-		if file_contents:
-			contents[file] = file_contents
+		contents[file] = f.read()
 
 print json.dumps(contents)"""
 		return pythonc_func % xunit_paths
@@ -221,19 +202,6 @@ class RemoteSetupCommand(RemoteCommand):
 		self.name = name
 
 
-class RemoteSshConfigCommand(RemoteSetupCommand):
-	def __init__(self, private_key):
-		super(RemoteSshConfigCommand, self).__init__('ssh-config')
-		self.private_key = private_key
-
-	def run(self, virtual_machine, output_handler=None):
-		# Explicitly ignore the output handler. We don't want to display this to the users.
-		return super(RemoteSshConfigCommand, self).run(virtual_machine)
-
-	def _run(self, virtual_machine, output_handler=None):
-		return virtual_machine.configure_ssh(self.private_key, output_handler=output_handler)
-
-
 class RemoteCheckoutCommand(RemoteSetupCommand):
 	def __init__(self, repo_name, repo_url, repo_type, ref):
 		super(RemoteCheckoutCommand, self).__init__(repo_type)
@@ -259,15 +227,13 @@ class RemotePatchCommand(RemoteSetupCommand):
 
 
 class RemoteProvisionCommand(RemoteSetupCommand):
-	def __init__(self, repo_name, environment, language_config, setup_config):
-		super(RemoteProvisionCommand, self).__init__('provision')
+	def __init__(self, repo_name, private_key):
+		super(RemoteProvisionCommand, self).__init__("provision")
 		self.repo_name = repo_name
-		self.environment = environment
-		self.language_config = language_config
-		self.setup_config = setup_config
+		self.private_key = private_key
 
 	def _run(self, virtual_machine, output_handler=None):
-		return virtual_machine.provision(self.repo_name, self.environment, self.language_config, self.setup_config, output_handler=output_handler)
+		return virtual_machine.provision(self.repo_name, self.private_key, output_handler=output_handler)
 
 
 class RemoteExportCommand(RemoteCommand):
@@ -292,8 +258,6 @@ class RemoteErrorCommand(RemoteCommand):
 		full_message = 'Error: %s' % self.error_message
 		return virtual_machine.ssh_call("echo -e %s; exit 1" % pipes.quote(full_message), output_handler=output_handler)
 
-	def get_xunit_contents(self):
-		pass
 
 class InvalidConfigurationException(Exception):
 	pass
diff --git a/ci/platform/virtual_machine/snapshotter/snapshotter.py b/ci/platform/virtual_machine/snapshotter/snapshotter.py
index fd87d26..37ae319 100644
--- a/ci/platform/virtual_machine/snapshotter/snapshotter.py
+++ b/ci/platform/virtual_machine/snapshotter/snapshotter.py
@@ -4,7 +4,6 @@ import time
 from collections import Counter
 
 import model_server
-import yaml
 
 from settings.store import StoreSettings
 from shared.constants import BuildStatus, VerificationUser
@@ -83,8 +82,6 @@ class Snapshotter(object):
 		try:
 			virtual_machine.wait_until_ready()
 
-			virtual_machine.configure_ssh(StoreSettings.ssh_private_key)
-
 			uri_translator = RepositoryUriTranslator()
 			self.clone_repositories(virtual_machine, repositories, uri_translator)
 
@@ -130,11 +127,7 @@ class Snapshotter(object):
 		self.logger.info('Provisioning for repository "%s" on branch "%s"' % (repository['name'], branch))
 		if virtual_machine.remote_checkout(repository['name'], uri_translator.translate(repository['uri']), repository['type'], branch).returncode != 0:
 			raise Exception('Failed to checkout branch "%s" for repository "%s"' % (branch, repository['name']))
-		config_contents_results = virtual_machine.ssh_call('cat ~/%s/*koality.yml' % repository['name'])
-		if config_contents_results.returncode != 0:
-			raise Exception('Could not find a koality.yml or .koality.yml file for branch "%s" for repository "%s"' % (branch, repository['name']))
-		config_contents = yaml.safe_load(config_contents_results.output)
-		provision_results = virtual_machine.provision(repository['name'], {}, config_contents.get('languages'), config_contents.get('setup'))
+		provision_results = virtual_machine.provision(repository['name'], StoreSettings.ssh_private_key)
 		if provision_results.returncode != 0:
 			failure_message = 'Provisioning failed with returncode %d' % provision_results.returncode
 			self.logger.error(failure_message + '\nProvision output:\n%s' % provision_results.output)
diff --git a/ci/platform/virtual_machine/virtual_machine.py b/ci/platform/virtual_machine/virtual_machine.py
index 103b867..ddd2d3b 100644
--- a/ci/platform/virtual_machine/virtual_machine.py
+++ b/ci/platform/virtual_machine/virtual_machine.py
@@ -1,16 +1,11 @@
 import pipes
-
-from util import greenlets
-
-import eventlet
-import eventlet.pools
-paramiko = eventlet.import_patched('paramiko')
+import uuid
 
 from database.engine import ConnectionFactory
-from pysh.shell_tools import ShellAnd, ShellCommand, ShellPipe, ShellAdvertised, ShellOr, ShellSilent, ShellChain, ShellRedirect, ShellIf, ShellNot, ShellTest, ShellSudo
-from provisioner import Provisioner
-from streaming_executor import StreamingExecutor, RemoteStreamingExecutor, CommandResults
+from shared.constants import VerificationUser
+from streaming_executor import StreamingExecutor
 from util.log import Logged
+from verification.pubkey_registrar import PubkeyRegistrar
 
 
 @Logged()
@@ -21,24 +16,9 @@ class VirtualMachine(object):
 		self.instance = instance
 		self.vm_username = vm_username
 		self._redis_conn = redis_connection or ConnectionFactory.get_redis_connection('virtual_machine')
-		self.ssh_pool = eventlet.pools.Pool(create=self._ssh_connect)
 
-	def provision(self, repo_name, environment, language_config, setup_config, output_handler=None):
-		try:
-			with self.ssh_pool.item() as ssh_conn:
-				provisioner = Provisioner(ssh_conn)
-				return provisioner.provision(
-					'~/%s' % repo_name,
-					environment=environment,
-					language_config=language_config,
-					setup_config=setup_config,
-					output_handler=output_handler
-				)
-		except:
-			failure_message = 'Failed to provision, could not connect to the testing instance.'
-			if output_handler is not None:
-				output_handler.append({1: failure_message})
-			return CommandResults(1, failure_message)
+	def provision(self, repo_name, private_key, output_handler=None):
+		raise NotImplementedError()
 
 	def export(self, export_prefix, file_paths, output_handler=None):
 		raise NotImplementedError("Currently only supported for EC2 VMs")
@@ -46,25 +26,11 @@ class VirtualMachine(object):
 	def ssh_args(self):
 		raise NotImplementedError()
 
-	def _ssh_connect(self):
-		ssh_conn = paramiko.SSHClient()
-		ssh_conn.set_missing_host_key_policy(paramiko.WarningPolicy())
-		ssh_args = self.ssh_args()
-		ssh_conn.connect(ssh_args.hostname, ssh_args.port, ssh_args.username)
-		return ssh_conn
-
 	def ssh_call(self, command, output_handler=None, timeout=None):
-		try:
-			with self.ssh_pool.item() as ssh_conn:
-				return RemoteStreamingExecutor(ssh_conn).execute(command, output_handler, timeout=timeout)
-		except Exception as e:
-			failure_message = 'Failed to connect to the testing instance: %s' % e
-			if output_handler is not None:
-				output_handler.append({1: failure_message})
-			return CommandResults(1, failure_message)
+		return self.call(self.ssh_args() + [str(command)], timeout=timeout, output_handler=output_handler)
 
 	def delete(self):
-		pass
+		raise NotImplementedError()
 
 	def rebuild(self):
 		raise NotImplementedError()
@@ -101,29 +67,10 @@ class VirtualMachine(object):
 		return {'instance_id': instance_id, 'username': username}
 
 	def _try_multiple_times(self, num_attempts, success_check, method, *args, **kwargs):
-		def shift_output_handler(output_handler, line_count_shift):
-			if output_handler is None:
-				return None
-
-			class ShiftedConsoleAppender(object):
-				def append(self, read_lines):
-					shifted_lines = dict(map(lambda item: (item[0] + line_count_shift, item[1]), read_lines.iteritems()))
-					output_handler.append(shifted_lines)
-
-			return ShiftedConsoleAppender()
-
-		output_handler = kwargs.get('output_handler')
-		sleep_time = kwargs.get('sleep_time', 5)
-		for attempt in xrange(num_attempts):
-			kwargs['output_handler'] = output_handler
+		for x in xrange(num_attempts):
 			results = method(*args, **kwargs)
 			if success_check(results):
 				return results
-			if attempt < num_attempts - 1:
-				output_handler = shift_output_handler(output_handler, results.output.count('\n') + 1)
-				self.logger.info('Sleeping %d seconds before retrying...' % sleep_time)
-				eventlet.sleep(sleep_time)
-				sleep_time *= 2
 		return results
 
 	def remote_patch(self, repo_name, patch_contents, output_handler=None):
@@ -132,148 +79,107 @@ class VirtualMachine(object):
 		ansi_reset = '\033[0m'
 
 		if patch_contents:
-			command = ShellAnd(
-				ShellCommand('echo %s' % pipes.quote('%sPATCH CONTENTS:%s' % (ansi_bright_cyan, ansi_reset))),
-				ShellCommand('echo'),
-				ShellCommand('echo %s' % pipes.quote(patch_contents)),
-				ShellCommand('echo'),
-				ShellCommand('echo %s' % pipes.quote('%sPATCHING:%s' % (ansi_bright_cyan, ansi_reset))),
-				ShellCommand('echo'),
-				ShellAdvertised('cd %s' % repo_name),
-				ShellAdvertised('patch -p1 ...', actual_command=ShellPipe('echo %s' % pipes.quote(patch_contents), 'patch -p1'))
-			)
+			command = ' && '.join((
+				'cd %s' % repo_name,
+				'echo %s' % pipes.quote('%sPATCH CONTENTS:%s' % (ansi_bright_cyan, ansi_reset)),
+				'echo',
+				'echo %s' % pipes.quote(patch_contents),
+				'echo',
+				'echo %s' % pipes.quote('%sPATCHING:%s' % (ansi_bright_cyan, ansi_reset)),
+				'echo',
+				'echo %s | patch -p1' % pipes.quote(patch_contents)
+			))
 		else:
 			command = 'echo %s' % pipes.quote('%sWARNING: No patch contents received.%s' % (ansi_bright_yellow, ansi_reset))
 
 		results = self.ssh_call(command, output_handler)
 		if results.returncode != 0:
-			self.logger.warn("Failed to apply patch %s\nResults: %s" % (patch_contents, results.output))
+			self.logger.warn("Failed to apply patch %s\nResults: " % (pipes.quote(patch_contents), results.output))
 		return results
 
-	def configure_ssh(self, private_key, output_handler=None):
-		try:
-			with self.ssh_pool.item() as ssh_conn:
-				provisioner = Provisioner(ssh_conn)
-				return provisioner.set_private_key(
-					private_key,
-					output_handler=output_handler
-				)
-		except:
-			failure_message = 'Failed to provision, could not connect to the testing instance.'
-			if output_handler is not None:
-				output_handler.append({1: failure_message})
-			return CommandResults(1, failure_message)
-
-	def _get_host_access_check_command(self, host_url):
-		return ShellAnd(
-			ShellCommand('echo Testing ssh connection to master instance...'),
-			ShellOr(
-				ShellAdvertised('ssh %s true' % host_url),
-				ShellAnd(
-					ShellCommand('echo Failed to access the master instance. Please check to make sure your security groups are configured correctly.'),
-					ShellCommand('false')
-				)
-			),
-		)
-
 	def remote_checkout(self, repo_name, repo_url, repo_type, ref, output_handler=None):
 		def _remote_fetch():
 			host_url = repo_url[:repo_url.find(":")]
-			command = ShellAnd(
-				self._get_host_access_check_command(host_url),
-				ShellOr(
-					ShellSilent('mv /repositories/cached/%s %s' % (repo_name, repo_name)),
-					ShellChain(
-						ShellSilent('rm -rf %s' % repo_name),
-						ShellAdvertised('git init %s' % repo_name)
-					)
-				),
-				ShellAdvertised('cd %s' % repo_name),
-				ShellAdvertised('git fetch %s %s -n --depth 1' % (repo_url, ref)),
-				ShellAdvertised('git checkout --force FETCH_HEAD')
-			)
-
-			results = self._try_multiple_times(5, lambda results: results.returncode == 0, self.ssh_call, command, output_handler=output_handler)
+			command = ' && '.join([
+				'(mv /repositories/cached/%s %s > /dev/null 2>&1 || (rm -rf %s > /dev/null 2>&1; git init %s))' % (repo_name, repo_name, repo_name, repo_name),
+				'ssh -oStrictHostKeyChecking=no %s true > /dev/null 2>&1' % host_url,  # Bypass the ssh yes/no prompt
+				'cd %s' % repo_name,
+				'git fetch %s %s -n --depth 1' % (repo_url, ref),
+				'git checkout --force FETCH_HEAD'])
+
+			results = self._try_multiple_times(5, lambda results: results.returncode == 0, self.ssh_call, command, output_handler)
 			if results.returncode != 0:
 				self.logger.error("Failed to check out ref %s from %s, results: %s" % (ref, repo_url, results))
 			return results
 
 		def _remote_update():
 			host_url, _, repo_uri = repo_url.split('://')[1].partition('/')
-
-			get_ref_command = ShellOr(
-				ShellAdvertised('hg update --clean %s' % ref),
-				ShellAnd(
-					ShellCommand('mkdir -p .hg/strip-backup'),
-					ShellCommand('echo Downloading bundle file for %s' % ref),
-					ShellPipe('ssh -q %s "hg cat-bundle %s %s"' % (host_url, repo_uri, ref), ShellRedirect('base64 -d', '.hg/strip-backup/%s.hg' % ref)),
-					ShellAdvertised('hg unbundle .hg/strip-backup/%s.hg' % ref),
-					ShellAdvertised('hg update --clean %s' % ref)
-				)
-			)
-
-			command = ShellAnd(
-				self._get_host_access_check_command(host_url),
-				ShellIf(
-					ShellSilent('mv /repositories/cached/%s %s' % (repo_name, repo_name)),
-					ShellAnd(
-						ShellAdvertised('cd %s' % repo_name),
-						ShellAdvertised('hg pull %s' % repo_url),
-						get_ref_command
-					),
-					ShellChain(
-						ShellSilent('rm -rf %s' % repo_name),
-						ShellAdvertised('hg clone --uncompressed %s %s' % (repo_url, repo_name)),
-						ShellCommand('cd %s' % repo_name),
-						get_ref_command
-					)
-				),
-
-			)
-
-			results = self._try_multiple_times(5, lambda results: results.returncode == 0, self.ssh_call, command, output_handler=output_handler)
+			command = ' && '.join([
+				'ssh -oStrictHostKeyChecking=no %s true > /dev/null 2>&1' % host_url,
+				'export PYTHONUNBUFFERED=true',
+				'(mv /repositories/cached/%s %s > /dev/null 2>&1 || (rm -rf %s > /dev/null 2>&1; hg clone --uncompressed %s %s))' % (repo_name, repo_name, repo_name, repo_url, repo_name),
+				'cd %s' % repo_name,
+				'hg pull %s' % repo_url,
+				'hg update --clean %s 2> /dev/null; r=$?; true' % ref,  # first try to check out the ref
+				'if [ "$r" == 0 ]; then exit 0; fi',
+				'mkdir -p .hg/strip-backup',  # otherwise try to get the ref from a bundle
+				'ssh -q %s \"hg cat-bundle %s %s\" | base64 -d > .hg/strip-backup/%s.hg' % (host_url, repo_uri, ref, ref),
+				'hg unbundle .hg/strip-backup/%s.hg' % ref,
+				'hg update --clean %s' % ref])
+
+			results = self._try_multiple_times(5, lambda results: results.returncode == 0, self.ssh_call, command, output_handler)
 			if results.returncode != 0:
 				self.logger.error("Failed to check out bundle %s from %s, results: %s" % (ref, repo_url, results))
 			return results
 
 		if repo_type == 'git':
 			assert isinstance(ref, (str, unicode))
-			return _remote_fetch()
+			return self._ssh_authorized(_remote_fetch, output_handler)
 		elif repo_type == 'hg':
-			return _remote_update()
+			return self._ssh_authorized(_remote_update, output_handler)
 		else:
 			self.logger.error("Unknown repository type in remote_checkout %s." % repo_type)
 
 	def remote_clone(self, repo_type, repo_name, repo_url, output_handler=None):
-		if repo_type == 'git':
-			host_url = repo_url[:repo_url.find(":")]
-			clone_flags = ''
-		elif repo_type == 'hg':
-			host_url, _, repo_uri = repo_url.split('://')[1].partition('/')
-			clone_flags = '--uncompressed'
-
-		command = ShellAnd(
-			ShellOr(
-				ShellNot(ShellTest('-e %s' % repo_name)),
-				ShellAdvertised('rm -rf %s' % repo_name)
-			),
-			self._get_host_access_check_command(host_url),
-			ShellAdvertised('%s clone %s %s %s' % (repo_type, clone_flags, repo_url, repo_name))
-		)
-		results = self._try_multiple_times(5, lambda results: results.returncode == 0, self.ssh_call, command, output_handler=output_handler)
-		if results.returncode != 0:
-			self.logger.error("Failed to clone %s, results: %s" % (repo_url, results))
-		return results
+		def _remote_clone():
+			if repo_type == 'git':
+				host_url = repo_url[:repo_url.find(":")]
+				clone_flags = ''
+			elif repo_type == 'hg':
+				host_url, _, repo_uri = repo_url.split('://')[1].partition('/')
+				clone_flags = '--uncompressed'
+
+			clone_command = '%s clone %s %s %s' % (repo_type, clone_flags, repo_url, repo_name)
+			command = ' && '.join([
+				'if [ -e %s ]; then rm -rf %s; fi' % (repo_name, repo_name),
+				'ssh -oStrictHostKeyChecking=no %s true > /dev/null 2>&1' % host_url,  # Bypass the ssh yes/no prompt
+				clone_command])
+			results = self._try_multiple_times(5, lambda results: results.returncode == 0, self.ssh_call, command, output_handler)
+			if results.returncode != 0:
+				self.logger.error("Failed to clone %s, results: %s" % (repo_url, results))
+			return results
+		return self._ssh_authorized(_remote_clone, output_handler)
+
+	def _ssh_authorized(self, authorized_command, output_handler=None):
+		generate_key = "mkdir ~/.ssh; yes | ssh-keygen -t rsa -N \"\" -f ~/.ssh/id_rsa"
+		retrieve_key = "(%s) > /dev/null 2>&1; cat ~/.ssh/id_rsa.pub" % generate_key
+		pubkey_results = self._try_multiple_times(5, lambda results: results.returncode == 0, self.ssh_call, retrieve_key, timeout=20)
+		if pubkey_results.returncode != 0:
+			if output_handler:
+				output_handler.append({1: "Failed to connect to the testing instance. Please try again."})
+			self.logger.error("Failed to set up ssh on vm at %s, results: %s" % (self.vm_id, pubkey_results))
+			return pubkey_results
+		pubkey = pubkey_results.output
+		alias = '__vm_%s:%s' % (self.vm_id, uuid.uuid1())
+		PubkeyRegistrar().register_pubkey(VerificationUser.id, alias, pubkey)
+		try:
+			return authorized_command()
+		finally:
+			PubkeyRegistrar().unregister_pubkey(VerificationUser.id, alias)
 
 	def cache_repository(self, repo_name, output_handler=None):
-		command = ShellChain(
-			ShellSudo(ShellCommand('chown -R %s:%s %s' % (self.vm_username, self.vm_username, repo_name))),
-			ShellOr(
-				ShellCommand('mv %s /repositories/cached/%s' % (repo_name, repo_name)),
-				ShellCommand('rm -rf %s' % repo_name)
-			)
-		)
-		return self.ssh_call(command, output_handler)
+		return self.ssh_call(';'.join(('sudo chown -R %s:%s %s' % (self.vm_username, self.vm_username, repo_name),
+			'mv %s /repositories/cached/%s || rm -rf %s' % (repo_name, repo_name, repo_name))), output_handler)
 
 	@classmethod
 	def get_newest_image(cls):
@@ -304,17 +210,6 @@ class VirtualMachine(object):
 	def __repr__(self):
 		return '%s(%r, %r, %r)' % (type(self).__name__, self.vm_id, self.instance, self.vm_username)
 
-	class SshArgs(object):
-		def __init__(self, username, hostname, port=22, options={}):
-			self.username = username
-			self.hostname = hostname
-			self.port = port
-			self.options = options
-
-		def to_arg_list(self):
-			return ['ssh'] + map(lambda option: '-o%s=%s' % option, self.options.iteritems()) + ['%s@%s' % (self.username, self.hostname), '-p', str(self.port)]
-
-
 	class ImageVersion(object):
 		"""A multiple decimal-place version string (such as 1.0.4)"""
 		def __init__(self, string_representation):
diff --git a/koality.yml b/koality.yml
index ec25c4e..dfd9692 100644
--- a/koality.yml
+++ b/koality.yml
@@ -4,9 +4,6 @@ languages:
   ruby: 1.9.3
   nodejs: 0.8.12
 setup:
-  - scripts:
-    # This is a temporary hack because pip is dumb
-    - rm -rf $VIRTUAL_ENV/lib/python2.7/site-packages/streaming_executor*
   - packages:
     - system:
       - zlib1g
@@ -34,7 +31,7 @@ setup:
       script: ./rabbitmq_setup.sh
 compile:
   scripts:
-    - pip install --upgrade .:
+    - python setup.py install:
         path: ci/platform
 test:
   machines: 3
@@ -42,7 +39,7 @@ test:
     - platform unit tests:
         path: ci/platform
         script: nosetests -sv --with-xunit tests/unit_tests
-        timeout: 10
+        timeout: 3
         xunit: nosetests.xml
   factories:
     - integration test factory:
